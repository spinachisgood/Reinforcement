{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 13.0  q_value: [0.0052359]   memory length: 13\n",
      "episode: 1   score: 10.0  q_value: [0.0052359]   memory length: 23\n",
      "episode: 2   score: 8.0  q_value: [0.0052359]   memory length: 31\n",
      "episode: 3   score: 9.0  q_value: [0.0052359]   memory length: 40\n",
      "episode: 4   score: 20.0  q_value: [0.0052359]   memory length: 60\n",
      "episode: 5   score: 15.0  q_value: [0.0052359]   memory length: 75\n",
      "episode: 6   score: 11.0  q_value: [0.0052359]   memory length: 86\n",
      "episode: 7   score: 10.0  q_value: [0.0052359]   memory length: 96\n",
      "episode: 8   score: 20.0  q_value: [0.0052359]   memory length: 116\n",
      "episode: 9   score: 14.0  q_value: [0.0052359]   memory length: 130\n",
      "episode: 10   score: 10.0  q_value: [0.0052359]   memory length: 140\n",
      "episode: 11   score: 9.0  q_value: [0.0052359]   memory length: 149\n",
      "episode: 12   score: 11.0  q_value: [0.0052359]   memory length: 160\n",
      "episode: 13   score: 9.0  q_value: [0.0052359]   memory length: 169\n",
      "episode: 14   score: 10.0  q_value: [0.0052359]   memory length: 179\n",
      "episode: 15   score: 13.0  q_value: [0.0052359]   memory length: 192\n",
      "episode: 16   score: 11.0  q_value: [0.0052359]   memory length: 203\n",
      "episode: 17   score: 10.0  q_value: [0.0052359]   memory length: 213\n",
      "episode: 18   score: 10.0  q_value: [0.0052359]   memory length: 223\n",
      "episode: 19   score: 10.0  q_value: [0.0052359]   memory length: 233\n",
      "episode: 20   score: 9.0  q_value: [0.0052359]   memory length: 242\n",
      "episode: 21   score: 8.0  q_value: [0.0052359]   memory length: 250\n",
      "episode: 22   score: 10.0  q_value: [0.0052359]   memory length: 260\n",
      "episode: 23   score: 9.0  q_value: [0.0052359]   memory length: 269\n",
      "episode: 24   score: 11.0  q_value: [0.0052359]   memory length: 280\n",
      "episode: 25   score: 8.0  q_value: [0.0052359]   memory length: 288\n",
      "episode: 26   score: 10.0  q_value: [0.0052359]   memory length: 298\n",
      "episode: 27   score: 18.0  q_value: [0.0052359]   memory length: 316\n",
      "episode: 28   score: 17.0  q_value: [0.0052359]   memory length: 333\n",
      "episode: 29   score: 24.0  q_value: [0.0052359]   memory length: 357\n",
      "episode: 30   score: 9.0  q_value: [0.0052359]   memory length: 366\n",
      "episode: 31   score: 10.0  q_value: [0.0052359]   memory length: 376\n",
      "episode: 32   score: 11.0  q_value: [0.0052359]   memory length: 387\n",
      "episode: 33   score: 9.0  q_value: [0.0052359]   memory length: 396\n",
      "episode: 34   score: 10.0  q_value: [0.0052359]   memory length: 406\n",
      "episode: 35   score: 8.0  q_value: [0.0052359]   memory length: 414\n",
      "episode: 36   score: 13.0  q_value: [0.0052359]   memory length: 427\n",
      "episode: 37   score: 10.0  q_value: [0.0052359]   memory length: 437\n",
      "episode: 38   score: 9.0  q_value: [0.0052359]   memory length: 446\n",
      "episode: 39   score: 10.0  q_value: [0.0052359]   memory length: 456\n",
      "episode: 40   score: 10.0  q_value: [0.0052359]   memory length: 466\n",
      "episode: 41   score: 10.0  q_value: [0.0052359]   memory length: 476\n",
      "episode: 42   score: 31.0  q_value: [0.0052359]   memory length: 507\n",
      "episode: 43   score: 10.0  q_value: [0.0052359]   memory length: 517\n",
      "episode: 44   score: 31.0  q_value: [0.0052359]   memory length: 548\n",
      "episode: 45   score: 32.0  q_value: [0.0052359]   memory length: 580\n",
      "episode: 46   score: 15.0  q_value: [0.0052359]   memory length: 595\n",
      "episode: 47   score: 11.0  q_value: [0.0052359]   memory length: 606\n",
      "episode: 48   score: 11.0  q_value: [0.0052359]   memory length: 617\n",
      "episode: 49   score: 15.0  q_value: [0.0052359]   memory length: 632\n",
      "episode: 50   score: 14.0  q_value: [0.0052359]   memory length: 646\n",
      "episode: 51   score: 10.0  q_value: [0.0052359]   memory length: 656\n",
      "episode: 52   score: 10.0  q_value: [0.0052359]   memory length: 666\n",
      "episode: 53   score: 10.0  q_value: [0.0052359]   memory length: 676\n",
      "episode: 54   score: 9.0  q_value: [0.0052359]   memory length: 685\n",
      "episode: 55   score: 10.0  q_value: [0.0052359]   memory length: 695\n",
      "episode: 56   score: 13.0  q_value: [0.0052359]   memory length: 708\n",
      "episode: 57   score: 27.0  q_value: [0.0052359]   memory length: 735\n",
      "episode: 58   score: 10.0  q_value: [0.0052359]   memory length: 745\n",
      "episode: 59   score: 24.0  q_value: [0.0052359]   memory length: 769\n",
      "episode: 60   score: 10.0  q_value: [0.0052359]   memory length: 779\n",
      "episode: 61   score: 15.0  q_value: [0.0052359]   memory length: 794\n",
      "episode: 62   score: 11.0  q_value: [0.0052359]   memory length: 805\n",
      "episode: 63   score: 9.0  q_value: [0.0052359]   memory length: 814\n",
      "episode: 64   score: 15.0  q_value: [0.0052359]   memory length: 829\n",
      "episode: 65   score: 34.0  q_value: [0.0052359]   memory length: 863\n",
      "episode: 66   score: 32.0  q_value: [0.0052359]   memory length: 895\n",
      "episode: 67   score: 10.0  q_value: [0.0052359]   memory length: 905\n",
      "episode: 68   score: 13.0  q_value: [0.0052359]   memory length: 918\n",
      "episode: 69   score: 26.0  q_value: [0.0052359]   memory length: 944\n",
      "episode: 70   score: 9.0  q_value: [0.0052359]   memory length: 953\n",
      "episode: 71   score: 10.0  q_value: [0.0052359]   memory length: 963\n",
      "episode: 72   score: 11.0  q_value: [0.0052359]   memory length: 974\n",
      "episode: 73   score: 9.0  q_value: [0.0052359]   memory length: 983\n",
      "episode: 74   score: 9.0  q_value: [0.0052359]   memory length: 992\n",
      "episode: 75   score: 10.0  q_value: [0.0052359]   memory length: 1002\n",
      "episode: 76   score: 10.0  q_value: [0.33879078]   memory length: 1012\n",
      "episode: 77   score: 10.0  q_value: [0.9917093]   memory length: 1022\n",
      "episode: 78   score: 10.0  q_value: [1.50939348]   memory length: 1032\n",
      "episode: 79   score: 8.0  q_value: [2.13387602]   memory length: 1040\n",
      "episode: 80   score: 9.0  q_value: [2.62673826]   memory length: 1049\n",
      "episode: 81   score: 8.0  q_value: [3.46567194]   memory length: 1057\n",
      "episode: 82   score: 12.0  q_value: [4.2560595]   memory length: 1069\n",
      "episode: 83   score: 11.0  q_value: [5.24687399]   memory length: 1080\n",
      "episode: 84   score: 10.0  q_value: [6.88040056]   memory length: 1090\n",
      "episode: 85   score: 9.0  q_value: [8.26925182]   memory length: 1099\n",
      "episode: 86   score: 14.0  q_value: [10.04440785]   memory length: 1113\n",
      "episode: 87   score: 19.0  q_value: [12.12539325]   memory length: 1132\n",
      "episode: 88   score: 14.0  q_value: [13.55845781]   memory length: 1146\n",
      "episode: 89   score: 10.0  q_value: [14.47501675]   memory length: 1156\n",
      "episode: 90   score: 41.0  q_value: [15.73947555]   memory length: 1197\n",
      "episode: 91   score: 12.0  q_value: [16.73592887]   memory length: 1209\n",
      "episode: 92   score: 11.0  q_value: [17.33853241]   memory length: 1220\n",
      "episode: 93   score: 13.0  q_value: [18.19232039]   memory length: 1233\n",
      "episode: 94   score: 16.0  q_value: [19.94764691]   memory length: 1249\n",
      "episode: 95   score: 11.0  q_value: [21.53043546]   memory length: 1260\n",
      "episode: 96   score: 14.0  q_value: [21.77883224]   memory length: 1274\n",
      "episode: 97   score: 15.0  q_value: [23.49560836]   memory length: 1289\n",
      "episode: 98   score: 11.0  q_value: [25.10585273]   memory length: 1300\n",
      "episode: 99   score: 19.0  q_value: [26.34397019]   memory length: 1319\n",
      "episode: 100   score: 16.0  q_value: [28.24291293]   memory length: 1335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 101   score: 10.0  q_value: [29.51773058]   memory length: 1345\n",
      "episode: 102   score: 48.0  q_value: [28.86633071]   memory length: 1393\n",
      "episode: 103   score: 35.0  q_value: [28.77507728]   memory length: 1428\n",
      "episode: 104   score: 10.0  q_value: [28.98226198]   memory length: 1438\n",
      "episode: 105   score: 14.0  q_value: [29.19281254]   memory length: 1452\n",
      "episode: 106   score: 13.0  q_value: [29.65355739]   memory length: 1465\n",
      "episode: 107   score: 17.0  q_value: [30.05919391]   memory length: 1482\n",
      "episode: 108   score: 13.0  q_value: [30.09657601]   memory length: 1495\n",
      "episode: 109   score: 16.0  q_value: [30.1452771]   memory length: 1511\n",
      "episode: 110   score: 55.0  q_value: [30.63140721]   memory length: 1566\n",
      "episode: 111   score: 39.0  q_value: [30.47557573]   memory length: 1605\n",
      "episode: 112   score: 118.0  q_value: [30.97413396]   memory length: 1723\n",
      "episode: 113   score: 83.0  q_value: [31.93144703]   memory length: 1806\n",
      "episode: 114   score: 96.0  q_value: [33.40636277]   memory length: 1902\n",
      "episode: 115   score: 44.0  q_value: [33.54057593]   memory length: 1946\n",
      "episode: 116   score: 50.0  q_value: [33.38481973]   memory length: 1996\n",
      "episode: 117   score: 179.0  q_value: [33.33697969]   memory length: 2175\n",
      "episode: 118   score: 76.0  q_value: [33.61896138]   memory length: 2251\n",
      "episode: 119   score: 59.0  q_value: [34.44056768]   memory length: 2310\n",
      "episode: 120   score: 41.0  q_value: [34.46333577]   memory length: 2351\n",
      "episode: 121   score: 42.0  q_value: [34.91666649]   memory length: 2393\n",
      "episode: 122   score: 57.0  q_value: [35.30498366]   memory length: 2450\n",
      "episode: 123   score: 175.0  q_value: [36.57007696]   memory length: 2625\n",
      "episode: 124   score: 115.0  q_value: [37.78787524]   memory length: 2740\n",
      "episode: 125   score: 106.0  q_value: [38.11960166]   memory length: 2846\n",
      "episode: 126   score: 105.0  q_value: [39.09966856]   memory length: 2951\n",
      "episode: 127   score: 159.0  q_value: [40.64715048]   memory length: 3110\n",
      "episode: 128   score: 121.0  q_value: [40.79413608]   memory length: 3231\n",
      "episode: 129   score: 200.0  q_value: [41.71326704]   memory length: 3431\n",
      "episode: 130   score: 162.0  q_value: [42.74983995]   memory length: 3593\n",
      "episode: 131   score: 107.0  q_value: [43.89239089]   memory length: 3700\n",
      "episode: 132   score: 153.0  q_value: [44.6701414]   memory length: 3853\n",
      "episode: 133   score: 200.0  q_value: [45.08880587]   memory length: 4053\n",
      "episode: 134   score: 200.0  q_value: [45.88781264]   memory length: 4253\n",
      "episode: 135   score: 150.0  q_value: [46.66025826]   memory length: 4403\n",
      "episode: 136   score: 126.0  q_value: [47.74008823]   memory length: 4529\n",
      "episode: 137   score: 200.0  q_value: [47.85137685]   memory length: 4729\n",
      "episode: 138   score: 200.0  q_value: [48.67621517]   memory length: 4929\n",
      "episode: 139   score: 158.0  q_value: [49.75985035]   memory length: 5087\n",
      "episode: 140   score: 200.0  q_value: [51.04576455]   memory length: 5287\n",
      "episode: 141   score: 200.0  q_value: [51.39675736]   memory length: 5487\n",
      "episode: 142   score: 200.0  q_value: [52.3115442]   memory length: 5687\n",
      "episode: 143   score: 148.0  q_value: [53.19309384]   memory length: 5835\n",
      "episode: 144   score: 142.0  q_value: [55.06153231]   memory length: 5977\n",
      "episode: 145   score: 200.0  q_value: [55.18748657]   memory length: 6177\n",
      "episode: 146   score: 166.0  q_value: [55.21666847]   memory length: 6343\n",
      "episode: 147   score: 200.0  q_value: [55.58470557]   memory length: 6543\n",
      "episode: 148   score: 195.0  q_value: [56.43307112]   memory length: 6738\n",
      "episode: 149   score: 179.0  q_value: [57.39799846]   memory length: 6917\n",
      "episode: 150   score: 135.0  q_value: [57.34007732]   memory length: 7052\n",
      "episode: 151   score: 200.0  q_value: [58.88874646]   memory length: 7252\n",
      "episode: 152   score: 127.0  q_value: [59.99674621]   memory length: 7379\n",
      "episode: 153   score: 200.0  q_value: [61.14504645]   memory length: 7500\n",
      "episode: 154   score: 169.0  q_value: [62.28111645]   memory length: 7500\n",
      "episode: 155   score: 136.0  q_value: [63.49819394]   memory length: 7500\n",
      "episode: 156   score: 200.0  q_value: [63.68058259]   memory length: 7500\n",
      "episode: 157   score: 109.0  q_value: [63.55818378]   memory length: 7500\n",
      "episode: 158   score: 200.0  q_value: [65.2481985]   memory length: 7500\n",
      "episode: 159   score: 132.0  q_value: [65.32031497]   memory length: 7500\n",
      "episode: 160   score: 140.0  q_value: [65.27210457]   memory length: 7500\n",
      "episode: 161   score: 191.0  q_value: [66.40811286]   memory length: 7500\n",
      "episode: 162   score: 153.0  q_value: [66.70743484]   memory length: 7500\n",
      "episode: 163   score: 170.0  q_value: [68.25732751]   memory length: 7500\n",
      "episode: 164   score: 132.0  q_value: [69.61398639]   memory length: 7500\n",
      "episode: 165   score: 190.0  q_value: [70.45802053]   memory length: 7500\n",
      "episode: 166   score: 200.0  q_value: [71.3515138]   memory length: 7500\n",
      "episode: 167   score: 155.0  q_value: [71.41808341]   memory length: 7500\n",
      "episode: 168   score: 200.0  q_value: [72.24326255]   memory length: 7500\n",
      "episode: 169   score: 137.0  q_value: [72.39459454]   memory length: 7500\n",
      "episode: 170   score: 143.0  q_value: [73.24452051]   memory length: 7500\n",
      "episode: 171   score: 200.0  q_value: [73.99673302]   memory length: 7500\n",
      "episode: 172   score: 121.0  q_value: [73.0979652]   memory length: 7500\n",
      "episode: 173   score: 188.0  q_value: [73.77005001]   memory length: 7500\n",
      "episode: 174   score: 171.0  q_value: [74.4958469]   memory length: 7500\n",
      "episode: 175   score: 200.0  q_value: [75.14780359]   memory length: 7500\n",
      "episode: 176   score: 176.0  q_value: [75.39433151]   memory length: 7500\n",
      "episode: 177   score: 200.0  q_value: [75.96309491]   memory length: 7500\n",
      "episode: 178   score: 200.0  q_value: [76.9931544]   memory length: 7500\n",
      "episode: 179   score: 200.0  q_value: [76.95960657]   memory length: 7500\n",
      "episode: 180   score: 200.0  q_value: [78.24528096]   memory length: 7500\n",
      "episode: 181   score: 200.0  q_value: [78.95633615]   memory length: 7500\n",
      "episode: 182   score: 200.0  q_value: [79.77796572]   memory length: 7500\n",
      "episode: 183   score: 115.0  q_value: [80.47596824]   memory length: 7500\n",
      "episode: 184   score: 164.0  q_value: [80.40298452]   memory length: 7500\n",
      "episode: 185   score: 157.0  q_value: [81.92263889]   memory length: 7500\n",
      "episode: 186   score: 180.0  q_value: [82.71207938]   memory length: 7500\n",
      "episode: 187   score: 200.0  q_value: [83.1703487]   memory length: 7500\n",
      "episode: 188   score: 200.0  q_value: [83.09607375]   memory length: 7500\n",
      "episode: 189   score: 179.0  q_value: [83.98340446]   memory length: 7500\n",
      "episode: 190   score: 200.0  q_value: [84.06242801]   memory length: 7500\n",
      "episode: 191   score: 181.0  q_value: [84.46722326]   memory length: 7500\n",
      "episode: 192   score: 200.0  q_value: [84.76553145]   memory length: 7500\n",
      "episode: 193   score: 200.0  q_value: [85.0882937]   memory length: 7500\n",
      "episode: 194   score: 200.0  q_value: [86.88772769]   memory length: 7500\n",
      "episode: 195   score: 200.0  q_value: [85.97763232]   memory length: 7500\n",
      "episode: 196   score: 200.0  q_value: [85.81156641]   memory length: 7500\n",
      "episode: 197   score: 200.0  q_value: [85.85738306]   memory length: 7500\n",
      "episode: 198   score: 200.0  q_value: [87.45014246]   memory length: 7500\n",
      "episode: 199   score: 176.0  q_value: [87.42150895]   memory length: 7500\n",
      "episode: 200   score: 200.0  q_value: [87.63073077]   memory length: 7500\n",
      "episode: 201   score: 200.0  q_value: [87.7396193]   memory length: 7500\n",
      "episode: 202   score: 200.0  q_value: [88.84595595]   memory length: 7500\n",
      "episode: 203   score: 200.0  q_value: [89.38282635]   memory length: 7500\n",
      "episode: 204   score: 200.0  q_value: [89.6874674]   memory length: 7500\n",
      "episode: 205   score: 200.0  q_value: [91.52822822]   memory length: 7500\n",
      "episode: 206   score: 200.0  q_value: [92.47510941]   memory length: 7500\n",
      "episode: 207   score: 200.0  q_value: [93.08107158]   memory length: 7500\n",
      "episode: 208   score: 200.0  q_value: [93.24249411]   memory length: 7500\n",
      "episode: 209   score: 200.0  q_value: [94.25739466]   memory length: 7500\n",
      "episode: 210   score: 200.0  q_value: [95.01004139]   memory length: 7500\n",
      "episode: 211   score: 200.0  q_value: [95.9605845]   memory length: 7500\n",
      "episode: 212   score: 180.0  q_value: [96.70526794]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 213   score: 200.0  q_value: [96.64780716]   memory length: 7500\n",
      "episode: 214   score: 200.0  q_value: [96.31918694]   memory length: 7500\n",
      "episode: 215   score: 200.0  q_value: [95.1321691]   memory length: 7500\n",
      "episode: 216   score: 200.0  q_value: [95.25266398]   memory length: 7500\n",
      "episode: 217   score: 200.0  q_value: [95.10990573]   memory length: 7500\n",
      "episode: 218   score: 200.0  q_value: [95.5418215]   memory length: 7500\n",
      "episode: 219   score: 200.0  q_value: [95.79922021]   memory length: 7500\n",
      "episode: 220   score: 200.0  q_value: [97.08853358]   memory length: 7500\n",
      "episode: 221   score: 200.0  q_value: [97.58403938]   memory length: 7500\n",
      "episode: 222   score: 200.0  q_value: [97.52701255]   memory length: 7500\n",
      "episode: 223   score: 200.0  q_value: [98.06494071]   memory length: 7500\n",
      "episode: 224   score: 200.0  q_value: [98.09635395]   memory length: 7500\n",
      "episode: 225   score: 200.0  q_value: [98.49879167]   memory length: 7500\n",
      "episode: 226   score: 200.0  q_value: [98.93653102]   memory length: 7500\n",
      "episode: 227   score: 200.0  q_value: [99.89763254]   memory length: 7500\n",
      "episode: 228   score: 200.0  q_value: [101.56346083]   memory length: 7500\n",
      "episode: 229   score: 200.0  q_value: [102.23006884]   memory length: 7500\n",
      "episode: 230   score: 200.0  q_value: [103.89062673]   memory length: 7500\n",
      "episode: 231   score: 200.0  q_value: [104.70854626]   memory length: 7500\n",
      "episode: 232   score: 200.0  q_value: [105.66759465]   memory length: 7500\n",
      "episode: 233   score: 200.0  q_value: [106.76658466]   memory length: 7500\n",
      "episode: 234   score: 200.0  q_value: [108.07733074]   memory length: 7500\n",
      "episode: 235   score: 200.0  q_value: [108.20880513]   memory length: 7500\n",
      "episode: 236   score: 200.0  q_value: [109.11596672]   memory length: 7500\n",
      "episode: 237   score: 200.0  q_value: [110.45766019]   memory length: 7500\n",
      "episode: 238   score: 200.0  q_value: [110.37963149]   memory length: 7500\n",
      "episode: 239   score: 200.0  q_value: [110.83233012]   memory length: 7500\n",
      "episode: 240   score: 200.0  q_value: [111.64615127]   memory length: 7500\n",
      "episode: 241   score: 188.0  q_value: [112.82559359]   memory length: 7500\n",
      "episode: 242   score: 200.0  q_value: [113.81976904]   memory length: 7500\n",
      "episode: 243   score: 200.0  q_value: [115.96625585]   memory length: 7500\n",
      "episode: 244   score: 200.0  q_value: [117.00135052]   memory length: 7500\n",
      "episode: 245   score: 200.0  q_value: [118.02931906]   memory length: 7500\n",
      "episode: 246   score: 200.0  q_value: [119.71890262]   memory length: 7500\n",
      "episode: 247   score: 200.0  q_value: [120.99196485]   memory length: 7500\n",
      "episode: 248   score: 200.0  q_value: [121.69759682]   memory length: 7500\n",
      "episode: 249   score: 200.0  q_value: [122.36839919]   memory length: 7500\n",
      "episode: 250   score: 200.0  q_value: [122.98338492]   memory length: 7500\n",
      "episode: 251   score: 200.0  q_value: [123.85598116]   memory length: 7500\n",
      "episode: 252   score: 200.0  q_value: [123.91790835]   memory length: 7500\n",
      "episode: 253   score: 200.0  q_value: [124.8262239]   memory length: 7500\n",
      "episode: 254   score: 200.0  q_value: [125.64983203]   memory length: 7500\n",
      "episode: 255   score: 200.0  q_value: [125.07347439]   memory length: 7500\n",
      "episode: 256   score: 200.0  q_value: [124.56633657]   memory length: 7500\n",
      "episode: 257   score: 200.0  q_value: [123.99313903]   memory length: 7500\n",
      "episode: 258   score: 200.0  q_value: [124.78223513]   memory length: 7500\n",
      "episode: 259   score: 200.0  q_value: [124.48631853]   memory length: 7500\n",
      "episode: 260   score: 200.0  q_value: [124.29482369]   memory length: 7500\n",
      "episode: 261   score: 200.0  q_value: [124.70858384]   memory length: 7500\n",
      "episode: 262   score: 200.0  q_value: [125.50122345]   memory length: 7500\n",
      "episode: 263   score: 200.0  q_value: [126.26244156]   memory length: 7500\n",
      "episode: 264   score: 200.0  q_value: [126.96099542]   memory length: 7500\n",
      "episode: 265   score: 200.0  q_value: [127.18950163]   memory length: 7500\n",
      "episode: 266   score: 200.0  q_value: [127.73682527]   memory length: 7500\n",
      "episode: 267   score: 200.0  q_value: [128.03220481]   memory length: 7500\n",
      "episode: 268   score: 200.0  q_value: [127.55278463]   memory length: 7500\n",
      "episode: 269   score: 10.0  q_value: [128.4574491]   memory length: 7500\n",
      "episode: 270   score: 163.0  q_value: [129.70561223]   memory length: 7500\n",
      "episode: 271   score: 10.0  q_value: [131.23739864]   memory length: 7500\n",
      "episode: 272   score: 13.0  q_value: [131.53712496]   memory length: 7500\n",
      "episode: 273   score: 10.0  q_value: [132.79291423]   memory length: 7500\n",
      "episode: 274   score: 9.0  q_value: [133.44845971]   memory length: 7500\n",
      "episode: 275   score: 9.0  q_value: [133.88876951]   memory length: 7500\n",
      "episode: 276   score: 8.0  q_value: [134.23395645]   memory length: 7500\n",
      "episode: 277   score: 10.0  q_value: [135.28447371]   memory length: 7500\n",
      "episode: 278   score: 45.0  q_value: [131.7362979]   memory length: 7500\n",
      "episode: 279   score: 200.0  q_value: [132.89075403]   memory length: 7500\n",
      "episode: 280   score: 200.0  q_value: [131.87253974]   memory length: 7500\n",
      "episode: 281   score: 200.0  q_value: [131.85954383]   memory length: 7500\n",
      "episode: 282   score: 200.0  q_value: [130.94913062]   memory length: 7500\n",
      "episode: 283   score: 200.0  q_value: [131.41706269]   memory length: 7500\n",
      "episode: 284   score: 200.0  q_value: [131.96620492]   memory length: 7500\n",
      "episode: 285   score: 200.0  q_value: [132.71729785]   memory length: 7500\n",
      "episode: 286   score: 200.0  q_value: [132.6914283]   memory length: 7500\n",
      "episode: 287   score: 200.0  q_value: [133.31196599]   memory length: 7500\n",
      "episode: 288   score: 200.0  q_value: [132.96314036]   memory length: 7500\n",
      "episode: 289   score: 200.0  q_value: [132.08190199]   memory length: 7500\n",
      "episode: 290   score: 200.0  q_value: [132.12987657]   memory length: 7500\n",
      "episode: 291   score: 200.0  q_value: [131.41957335]   memory length: 7500\n",
      "episode: 292   score: 200.0  q_value: [132.2898944]   memory length: 7500\n",
      "episode: 293   score: 200.0  q_value: [132.03042583]   memory length: 7500\n",
      "episode: 294   score: 200.0  q_value: [130.38761365]   memory length: 7500\n",
      "episode: 295   score: 200.0  q_value: [130.0844046]   memory length: 7500\n",
      "episode: 296   score: 200.0  q_value: [130.8282425]   memory length: 7500\n",
      "episode: 297   score: 48.0  q_value: [131.45610057]   memory length: 7500\n",
      "episode: 298   score: 200.0  q_value: [130.76766742]   memory length: 7500\n",
      "episode: 299   score: 200.0  q_value: [130.19984281]   memory length: 7500\n",
      "episode: 300   score: 200.0  q_value: [130.60768288]   memory length: 7500\n",
      "episode: 301   score: 200.0  q_value: [130.61191153]   memory length: 7500\n",
      "episode: 302   score: 200.0  q_value: [130.21809485]   memory length: 7500\n",
      "episode: 303   score: 200.0  q_value: [130.13340318]   memory length: 7500\n",
      "episode: 304   score: 200.0  q_value: [129.58434862]   memory length: 7500\n",
      "episode: 305   score: 200.0  q_value: [129.81076929]   memory length: 7500\n",
      "episode: 306   score: 200.0  q_value: [131.25991399]   memory length: 7500\n",
      "episode: 307   score: 200.0  q_value: [132.43287692]   memory length: 7500\n",
      "episode: 308   score: 200.0  q_value: [133.97058423]   memory length: 7500\n",
      "episode: 309   score: 200.0  q_value: [133.08076362]   memory length: 7500\n",
      "episode: 310   score: 200.0  q_value: [132.12449912]   memory length: 7500\n",
      "episode: 311   score: 200.0  q_value: [132.61460618]   memory length: 7500\n",
      "episode: 312   score: 200.0  q_value: [132.20879158]   memory length: 7500\n",
      "episode: 313   score: 200.0  q_value: [131.76290867]   memory length: 7500\n",
      "episode: 314   score: 200.0  q_value: [130.64624935]   memory length: 7500\n",
      "episode: 315   score: 200.0  q_value: [130.06226713]   memory length: 7500\n",
      "episode: 316   score: 200.0  q_value: [130.36441145]   memory length: 7500\n",
      "episode: 317   score: 200.0  q_value: [130.95863084]   memory length: 7500\n",
      "episode: 318   score: 200.0  q_value: [132.04317586]   memory length: 7500\n",
      "episode: 319   score: 200.0  q_value: [132.76220478]   memory length: 7500\n",
      "episode: 320   score: 200.0  q_value: [132.63972674]   memory length: 7500\n",
      "episode: 321   score: 200.0  q_value: [134.22266897]   memory length: 7500\n",
      "episode: 322   score: 200.0  q_value: [133.60529522]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 323   score: 200.0  q_value: [134.35990304]   memory length: 7500\n",
      "episode: 324   score: 200.0  q_value: [136.28843231]   memory length: 7500\n",
      "episode: 325   score: 200.0  q_value: [136.14539638]   memory length: 7500\n",
      "episode: 326   score: 200.0  q_value: [137.38160471]   memory length: 7500\n",
      "episode: 327   score: 200.0  q_value: [139.74356582]   memory length: 7500\n",
      "episode: 328   score: 200.0  q_value: [139.96772615]   memory length: 7500\n",
      "episode: 329   score: 200.0  q_value: [138.80140252]   memory length: 7500\n",
      "episode: 330   score: 200.0  q_value: [139.18199512]   memory length: 7500\n",
      "episode: 331   score: 169.0  q_value: [139.06318427]   memory length: 7500\n",
      "episode: 332   score: 200.0  q_value: [138.58757959]   memory length: 7500\n",
      "episode: 333   score: 200.0  q_value: [138.62782157]   memory length: 7500\n",
      "episode: 334   score: 167.0  q_value: [141.21453006]   memory length: 7500\n",
      "episode: 335   score: 177.0  q_value: [141.56660105]   memory length: 7500\n",
      "episode: 336   score: 177.0  q_value: [140.86858114]   memory length: 7500\n",
      "episode: 337   score: 171.0  q_value: [140.95352272]   memory length: 7500\n",
      "episode: 338   score: 171.0  q_value: [141.86287857]   memory length: 7500\n",
      "episode: 339   score: 191.0  q_value: [143.80314774]   memory length: 7500\n",
      "episode: 340   score: 200.0  q_value: [145.35020425]   memory length: 7500\n",
      "episode: 341   score: 200.0  q_value: [146.15030052]   memory length: 7500\n",
      "episode: 342   score: 200.0  q_value: [146.5649268]   memory length: 7500\n",
      "episode: 343   score: 200.0  q_value: [147.12771282]   memory length: 7500\n",
      "episode: 344   score: 200.0  q_value: [147.79681845]   memory length: 7500\n",
      "episode: 345   score: 200.0  q_value: [147.72654234]   memory length: 7500\n",
      "episode: 346   score: 200.0  q_value: [148.94948497]   memory length: 7500\n",
      "episode: 347   score: 200.0  q_value: [149.60735041]   memory length: 7500\n",
      "episode: 348   score: 200.0  q_value: [149.58880834]   memory length: 7500\n",
      "episode: 349   score: 200.0  q_value: [150.57465325]   memory length: 7500\n",
      "episode: 350   score: 200.0  q_value: [150.3626228]   memory length: 7500\n",
      "episode: 351   score: 200.0  q_value: [152.00783895]   memory length: 7500\n",
      "episode: 352   score: 200.0  q_value: [151.6024534]   memory length: 7500\n",
      "episode: 353   score: 200.0  q_value: [153.52112793]   memory length: 7500\n",
      "episode: 354   score: 200.0  q_value: [154.24334338]   memory length: 7500\n",
      "episode: 355   score: 200.0  q_value: [156.25142335]   memory length: 7500\n",
      "episode: 356   score: 200.0  q_value: [159.02887142]   memory length: 7500\n",
      "episode: 357   score: 200.0  q_value: [159.10277659]   memory length: 7500\n",
      "episode: 358   score: 200.0  q_value: [159.92995623]   memory length: 7500\n",
      "episode: 359   score: 184.0  q_value: [160.03395077]   memory length: 7500\n",
      "episode: 360   score: 175.0  q_value: [160.06856545]   memory length: 7500\n",
      "episode: 361   score: 171.0  q_value: [160.96769068]   memory length: 7500\n",
      "episode: 362   score: 194.0  q_value: [162.71323152]   memory length: 7500\n",
      "episode: 363   score: 200.0  q_value: [162.69386969]   memory length: 7500\n",
      "episode: 364   score: 200.0  q_value: [162.98128569]   memory length: 7500\n",
      "episode: 365   score: 179.0  q_value: [162.9845286]   memory length: 7500\n",
      "episode: 366   score: 173.0  q_value: [161.41934921]   memory length: 7500\n",
      "episode: 367   score: 141.0  q_value: [161.25643301]   memory length: 7500\n",
      "episode: 368   score: 167.0  q_value: [161.60053506]   memory length: 7500\n",
      "episode: 369   score: 180.0  q_value: [162.04795674]   memory length: 7500\n",
      "episode: 370   score: 200.0  q_value: [161.72667744]   memory length: 7500\n",
      "episode: 371   score: 200.0  q_value: [162.87057206]   memory length: 7500\n",
      "episode: 372   score: 200.0  q_value: [161.74873477]   memory length: 7500\n",
      "episode: 373   score: 200.0  q_value: [161.04093286]   memory length: 7500\n",
      "episode: 374   score: 200.0  q_value: [162.07302171]   memory length: 7500\n",
      "episode: 375   score: 200.0  q_value: [162.16503206]   memory length: 7500\n",
      "episode: 376   score: 200.0  q_value: [161.88462777]   memory length: 7500\n",
      "episode: 377   score: 200.0  q_value: [160.78796024]   memory length: 7500\n",
      "episode: 378   score: 200.0  q_value: [159.9630183]   memory length: 7500\n",
      "episode: 379   score: 180.0  q_value: [159.24732751]   memory length: 7500\n",
      "episode: 380   score: 200.0  q_value: [159.75526182]   memory length: 7500\n",
      "episode: 381   score: 200.0  q_value: [161.81037713]   memory length: 7500\n",
      "episode: 382   score: 173.0  q_value: [161.82088618]   memory length: 7500\n",
      "episode: 383   score: 159.0  q_value: [162.045213]   memory length: 7500\n",
      "episode: 384   score: 168.0  q_value: [162.3446331]   memory length: 7500\n",
      "episode: 385   score: 200.0  q_value: [163.32092032]   memory length: 7500\n",
      "episode: 386   score: 200.0  q_value: [163.80788287]   memory length: 7500\n",
      "episode: 387   score: 178.0  q_value: [164.86122915]   memory length: 7500\n",
      "episode: 388   score: 200.0  q_value: [164.30150342]   memory length: 7500\n",
      "episode: 389   score: 183.0  q_value: [164.41587983]   memory length: 7500\n",
      "episode: 390   score: 176.0  q_value: [164.73171339]   memory length: 7500\n",
      "episode: 391   score: 200.0  q_value: [165.17959933]   memory length: 7500\n",
      "episode: 392   score: 189.0  q_value: [165.78952997]   memory length: 7500\n",
      "episode: 393   score: 153.0  q_value: [166.94437575]   memory length: 7500\n",
      "episode: 394   score: 198.0  q_value: [167.37251779]   memory length: 7500\n",
      "episode: 395   score: 168.0  q_value: [167.32426552]   memory length: 7500\n",
      "episode: 396   score: 168.0  q_value: [167.67902645]   memory length: 7500\n",
      "episode: 397   score: 183.0  q_value: [167.82633319]   memory length: 7500\n",
      "episode: 398   score: 169.0  q_value: [168.46654106]   memory length: 7500\n",
      "episode: 399   score: 177.0  q_value: [169.07934979]   memory length: 7500\n",
      "episode: 400   score: 67.0  q_value: [169.80416042]   memory length: 7500\n",
      "episode: 401   score: 181.0  q_value: [171.97760316]   memory length: 7500\n",
      "episode: 402   score: 109.0  q_value: [173.34119392]   memory length: 7500\n",
      "episode: 403   score: 122.0  q_value: [172.37070951]   memory length: 7500\n",
      "episode: 404   score: 108.0  q_value: [175.1902556]   memory length: 7500\n",
      "episode: 405   score: 200.0  q_value: [176.9659549]   memory length: 7500\n",
      "episode: 406   score: 188.0  q_value: [177.16361107]   memory length: 7500\n",
      "episode: 407   score: 83.0  q_value: [175.79683249]   memory length: 7500\n",
      "episode: 408   score: 80.0  q_value: [177.67960526]   memory length: 7500\n",
      "episode: 409   score: 23.0  q_value: [178.22027722]   memory length: 7500\n",
      "episode: 410   score: 88.0  q_value: [179.61224612]   memory length: 7500\n",
      "episode: 411   score: 86.0  q_value: [181.20922544]   memory length: 7500\n",
      "episode: 412   score: 92.0  q_value: [179.08482468]   memory length: 7500\n",
      "episode: 413   score: 165.0  q_value: [178.70128988]   memory length: 7500\n",
      "episode: 414   score: 153.0  q_value: [177.24568237]   memory length: 7500\n",
      "episode: 415   score: 143.0  q_value: [177.62069006]   memory length: 7500\n",
      "episode: 416   score: 185.0  q_value: [177.45017115]   memory length: 7500\n",
      "episode: 417   score: 98.0  q_value: [179.11229156]   memory length: 7500\n",
      "episode: 418   score: 197.0  q_value: [180.97658902]   memory length: 7500\n",
      "episode: 419   score: 168.0  q_value: [178.58669051]   memory length: 7500\n",
      "episode: 420   score: 194.0  q_value: [180.22900077]   memory length: 7500\n",
      "episode: 421   score: 169.0  q_value: [178.84901992]   memory length: 7500\n",
      "episode: 422   score: 200.0  q_value: [179.68978641]   memory length: 7500\n",
      "episode: 423   score: 200.0  q_value: [179.79527059]   memory length: 7500\n",
      "episode: 424   score: 200.0  q_value: [179.74813277]   memory length: 7500\n",
      "episode: 425   score: 200.0  q_value: [180.59211369]   memory length: 7500\n",
      "episode: 426   score: 156.0  q_value: [181.26467154]   memory length: 7500\n",
      "episode: 427   score: 200.0  q_value: [182.63189559]   memory length: 7500\n",
      "episode: 428   score: 200.0  q_value: [179.76873719]   memory length: 7500\n",
      "episode: 429   score: 104.0  q_value: [180.93436313]   memory length: 7500\n",
      "episode: 430   score: 120.0  q_value: [180.9337404]   memory length: 7500\n",
      "episode: 431   score: 156.0  q_value: [182.52569984]   memory length: 7500\n",
      "episode: 432   score: 200.0  q_value: [184.38952919]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 433   score: 157.0  q_value: [185.18671552]   memory length: 7500\n",
      "episode: 434   score: 159.0  q_value: [186.27997728]   memory length: 7500\n",
      "episode: 435   score: 155.0  q_value: [186.96115562]   memory length: 7500\n",
      "episode: 436   score: 200.0  q_value: [186.44562036]   memory length: 7500\n",
      "episode: 437   score: 200.0  q_value: [188.19842626]   memory length: 7500\n",
      "episode: 438   score: 200.0  q_value: [189.75952493]   memory length: 7500\n",
      "episode: 439   score: 200.0  q_value: [191.54143588]   memory length: 7500\n",
      "episode: 440   score: 200.0  q_value: [191.98316406]   memory length: 7500\n",
      "episode: 441   score: 200.0  q_value: [192.65102192]   memory length: 7500\n",
      "episode: 442   score: 200.0  q_value: [193.40078803]   memory length: 7500\n",
      "episode: 443   score: 200.0  q_value: [195.1969192]   memory length: 7500\n",
      "episode: 444   score: 200.0  q_value: [195.84867047]   memory length: 7500\n",
      "episode: 445   score: 200.0  q_value: [197.49336394]   memory length: 7500\n",
      "episode: 446   score: 200.0  q_value: [200.96741482]   memory length: 7500\n",
      "episode: 447   score: 196.0  q_value: [202.57434293]   memory length: 7500\n",
      "episode: 448   score: 200.0  q_value: [205.09329756]   memory length: 7500\n",
      "episode: 449   score: 200.0  q_value: [203.51368277]   memory length: 7500\n",
      "episode: 450   score: 200.0  q_value: [204.15419429]   memory length: 7500\n",
      "episode: 451   score: 200.0  q_value: [204.68840054]   memory length: 7500\n",
      "episode: 452   score: 200.0  q_value: [204.1315081]   memory length: 7500\n",
      "episode: 453   score: 200.0  q_value: [204.75018784]   memory length: 7500\n",
      "episode: 454   score: 200.0  q_value: [206.298365]   memory length: 7500\n",
      "episode: 455   score: 199.0  q_value: [208.91716329]   memory length: 7500\n",
      "episode: 456   score: 166.0  q_value: [210.38948846]   memory length: 7500\n",
      "episode: 457   score: 157.0  q_value: [214.43560337]   memory length: 7500\n",
      "episode: 458   score: 136.0  q_value: [216.98020846]   memory length: 7500\n",
      "episode: 459   score: 127.0  q_value: [220.36469071]   memory length: 7500\n",
      "episode: 460   score: 130.0  q_value: [223.89303439]   memory length: 7500\n",
      "episode: 461   score: 148.0  q_value: [228.49308254]   memory length: 7500\n",
      "episode: 462   score: 147.0  q_value: [233.73183302]   memory length: 7500\n",
      "episode: 463   score: 134.0  q_value: [238.37319152]   memory length: 7500\n",
      "episode: 464   score: 134.0  q_value: [242.62935424]   memory length: 7500\n",
      "episode: 465   score: 136.0  q_value: [246.74301822]   memory length: 7500\n",
      "episode: 466   score: 161.0  q_value: [251.7895792]   memory length: 7500\n",
      "episode: 467   score: 166.0  q_value: [253.47781181]   memory length: 7500\n",
      "episode: 468   score: 158.0  q_value: [255.65354285]   memory length: 7500\n",
      "episode: 469   score: 154.0  q_value: [259.84462299]   memory length: 7500\n",
      "episode: 470   score: 155.0  q_value: [260.72691774]   memory length: 7500\n",
      "episode: 471   score: 145.0  q_value: [261.87001615]   memory length: 7500\n",
      "episode: 472   score: 150.0  q_value: [264.13625167]   memory length: 7500\n",
      "episode: 473   score: 166.0  q_value: [266.75675098]   memory length: 7500\n",
      "episode: 474   score: 166.0  q_value: [267.92251567]   memory length: 7500\n",
      "episode: 475   score: 188.0  q_value: [269.4262644]   memory length: 7500\n",
      "episode: 476   score: 175.0  q_value: [270.39432898]   memory length: 7500\n",
      "episode: 477   score: 178.0  q_value: [272.38630457]   memory length: 7500\n",
      "episode: 478   score: 184.0  q_value: [273.86880355]   memory length: 7500\n",
      "episode: 479   score: 200.0  q_value: [272.15334288]   memory length: 7500\n",
      "episode: 480   score: 163.0  q_value: [272.7157415]   memory length: 7500\n",
      "episode: 481   score: 200.0  q_value: [272.95241344]   memory length: 7500\n",
      "episode: 482   score: 191.0  q_value: [274.9084287]   memory length: 7500\n",
      "episode: 483   score: 200.0  q_value: [276.02360067]   memory length: 7500\n",
      "episode: 484   score: 200.0  q_value: [273.65170851]   memory length: 7500\n",
      "episode: 485   score: 200.0  q_value: [274.56147305]   memory length: 7500\n",
      "episode: 486   score: 200.0  q_value: [274.32884997]   memory length: 7500\n",
      "episode: 487   score: 200.0  q_value: [273.36433599]   memory length: 7500\n",
      "episode: 488   score: 200.0  q_value: [273.27921304]   memory length: 7500\n",
      "episode: 489   score: 200.0  q_value: [272.6927413]   memory length: 7500\n",
      "episode: 490   score: 200.0  q_value: [273.39250134]   memory length: 7500\n",
      "episode: 491   score: 200.0  q_value: [273.6480497]   memory length: 7500\n",
      "episode: 492   score: 200.0  q_value: [273.9674611]   memory length: 7500\n",
      "episode: 493   score: 200.0  q_value: [273.5105899]   memory length: 7500\n",
      "episode: 494   score: 200.0  q_value: [273.41443374]   memory length: 7500\n",
      "episode: 495   score: 200.0  q_value: [273.42131735]   memory length: 7500\n",
      "episode: 496   score: 200.0  q_value: [273.00646349]   memory length: 7500\n",
      "episode: 497   score: 200.0  q_value: [272.89169437]   memory length: 7500\n",
      "episode: 498   score: 200.0  q_value: [273.34545313]   memory length: 7500\n",
      "episode: 499   score: 200.0  q_value: [273.61386266]   memory length: 7500\n",
      "episode: 500   score: 200.0  q_value: [272.47586678]   memory length: 7500\n",
      "episode: 501   score: 200.0  q_value: [273.05700519]   memory length: 7500\n",
      "episode: 502   score: 200.0  q_value: [273.4129666]   memory length: 7500\n",
      "episode: 503   score: 200.0  q_value: [274.48936618]   memory length: 7500\n",
      "episode: 504   score: 200.0  q_value: [276.23039687]   memory length: 7500\n",
      "episode: 505   score: 200.0  q_value: [277.33199244]   memory length: 7500\n",
      "episode: 506   score: 200.0  q_value: [277.33094211]   memory length: 7500\n",
      "episode: 507   score: 163.0  q_value: [277.52536464]   memory length: 7500\n",
      "episode: 508   score: 138.0  q_value: [277.07504175]   memory length: 7500\n",
      "episode: 509   score: 12.0  q_value: [279.63505783]   memory length: 7500\n",
      "episode: 510   score: 142.0  q_value: [280.8598982]   memory length: 7500\n",
      "episode: 511   score: 14.0  q_value: [283.42667992]   memory length: 7500\n",
      "episode: 512   score: 155.0  q_value: [283.09209097]   memory length: 7500\n",
      "episode: 513   score: 163.0  q_value: [285.32601369]   memory length: 7500\n",
      "episode: 514   score: 144.0  q_value: [287.02080784]   memory length: 7500\n",
      "episode: 515   score: 185.0  q_value: [288.84568885]   memory length: 7500\n",
      "episode: 516   score: 178.0  q_value: [288.03503154]   memory length: 7500\n",
      "episode: 517   score: 161.0  q_value: [288.65987264]   memory length: 7500\n",
      "episode: 518   score: 200.0  q_value: [290.26451216]   memory length: 7500\n",
      "episode: 519   score: 200.0  q_value: [290.26397011]   memory length: 7500\n",
      "episode: 520   score: 200.0  q_value: [288.43965748]   memory length: 7500\n",
      "episode: 521   score: 200.0  q_value: [286.4746277]   memory length: 7500\n",
      "episode: 522   score: 200.0  q_value: [285.96316561]   memory length: 7500\n",
      "episode: 523   score: 178.0  q_value: [284.11379807]   memory length: 7500\n",
      "episode: 524   score: 200.0  q_value: [283.26366874]   memory length: 7500\n",
      "episode: 525   score: 200.0  q_value: [282.10655464]   memory length: 7500\n",
      "episode: 526   score: 200.0  q_value: [280.35897643]   memory length: 7500\n",
      "episode: 527   score: 200.0  q_value: [280.779881]   memory length: 7500\n",
      "episode: 528   score: 200.0  q_value: [281.50272747]   memory length: 7500\n",
      "episode: 529   score: 200.0  q_value: [280.98287591]   memory length: 7500\n",
      "episode: 530   score: 200.0  q_value: [283.36912557]   memory length: 7500\n",
      "episode: 531   score: 200.0  q_value: [284.12086466]   memory length: 7500\n",
      "episode: 532   score: 200.0  q_value: [284.8878316]   memory length: 7500\n",
      "episode: 533   score: 200.0  q_value: [283.43645825]   memory length: 7500\n",
      "episode: 534   score: 200.0  q_value: [284.18454133]   memory length: 7500\n",
      "episode: 535   score: 200.0  q_value: [283.55930332]   memory length: 7500\n",
      "episode: 536   score: 177.0  q_value: [282.67133775]   memory length: 7500\n",
      "episode: 537   score: 196.0  q_value: [282.88550672]   memory length: 7500\n",
      "episode: 538   score: 200.0  q_value: [283.57937574]   memory length: 7500\n",
      "episode: 539   score: 200.0  q_value: [284.50343857]   memory length: 7500\n",
      "episode: 540   score: 200.0  q_value: [284.56085329]   memory length: 7500\n",
      "episode: 541   score: 200.0  q_value: [284.28783367]   memory length: 7500\n",
      "episode: 542   score: 200.0  q_value: [284.62742334]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 543   score: 187.0  q_value: [284.271971]   memory length: 7500\n",
      "episode: 544   score: 185.0  q_value: [283.84111053]   memory length: 7500\n",
      "episode: 545   score: 196.0  q_value: [283.41869032]   memory length: 7500\n",
      "episode: 546   score: 200.0  q_value: [283.15798311]   memory length: 7500\n",
      "episode: 547   score: 200.0  q_value: [281.99300477]   memory length: 7500\n",
      "episode: 548   score: 182.0  q_value: [279.28869046]   memory length: 7500\n",
      "episode: 549   score: 200.0  q_value: [278.85015734]   memory length: 7500\n",
      "episode: 550   score: 200.0  q_value: [279.39235475]   memory length: 7500\n",
      "episode: 551   score: 200.0  q_value: [280.62603241]   memory length: 7500\n",
      "episode: 552   score: 187.0  q_value: [279.94089427]   memory length: 7500\n",
      "episode: 553   score: 163.0  q_value: [280.85548055]   memory length: 7500\n",
      "episode: 554   score: 172.0  q_value: [280.32701574]   memory length: 7500\n",
      "episode: 555   score: 200.0  q_value: [280.36160236]   memory length: 7500\n",
      "episode: 556   score: 200.0  q_value: [281.35500302]   memory length: 7500\n",
      "episode: 557   score: 200.0  q_value: [280.09177328]   memory length: 7500\n",
      "episode: 558   score: 182.0  q_value: [280.78396186]   memory length: 7500\n",
      "episode: 559   score: 195.0  q_value: [280.70344256]   memory length: 7500\n",
      "episode: 560   score: 191.0  q_value: [281.83660921]   memory length: 7500\n",
      "episode: 561   score: 178.0  q_value: [282.86473332]   memory length: 7500\n",
      "episode: 562   score: 198.0  q_value: [282.90146616]   memory length: 7500\n",
      "episode: 563   score: 200.0  q_value: [284.86527728]   memory length: 7500\n",
      "episode: 564   score: 200.0  q_value: [284.7499437]   memory length: 7500\n",
      "episode: 565   score: 200.0  q_value: [283.99693305]   memory length: 7500\n",
      "episode: 566   score: 200.0  q_value: [283.86598691]   memory length: 7500\n",
      "episode: 567   score: 200.0  q_value: [284.30036521]   memory length: 7500\n",
      "episode: 568   score: 200.0  q_value: [283.95946973]   memory length: 7500\n",
      "episode: 569   score: 200.0  q_value: [283.93113851]   memory length: 7500\n",
      "episode: 570   score: 200.0  q_value: [284.24915355]   memory length: 7500\n",
      "episode: 571   score: 200.0  q_value: [283.79907645]   memory length: 7500\n",
      "episode: 572   score: 200.0  q_value: [282.84546486]   memory length: 7500\n",
      "episode: 573   score: 200.0  q_value: [282.63801635]   memory length: 7500\n",
      "episode: 574   score: 200.0  q_value: [282.16569674]   memory length: 7500\n",
      "episode: 575   score: 200.0  q_value: [282.44009662]   memory length: 7500\n",
      "episode: 576   score: 200.0  q_value: [280.90383896]   memory length: 7500\n",
      "episode: 577   score: 200.0  q_value: [281.17256756]   memory length: 7500\n",
      "episode: 578   score: 200.0  q_value: [280.56878259]   memory length: 7500\n",
      "episode: 579   score: 200.0  q_value: [280.19797529]   memory length: 7500\n",
      "episode: 580   score: 200.0  q_value: [279.77563713]   memory length: 7500\n",
      "episode: 581   score: 200.0  q_value: [281.46989618]   memory length: 7500\n",
      "episode: 582   score: 200.0  q_value: [283.17603772]   memory length: 7500\n",
      "episode: 583   score: 200.0  q_value: [285.31258974]   memory length: 7500\n",
      "episode: 584   score: 200.0  q_value: [287.26156784]   memory length: 7500\n",
      "episode: 585   score: 200.0  q_value: [289.84857307]   memory length: 7500\n",
      "episode: 586   score: 196.0  q_value: [291.25588447]   memory length: 7500\n",
      "episode: 587   score: 177.0  q_value: [292.31191653]   memory length: 7500\n",
      "episode: 588   score: 200.0  q_value: [291.49016434]   memory length: 7500\n",
      "episode: 589   score: 160.0  q_value: [290.20040947]   memory length: 7500\n",
      "episode: 590   score: 161.0  q_value: [290.53568014]   memory length: 7500\n",
      "episode: 591   score: 146.0  q_value: [289.83974344]   memory length: 7500\n",
      "episode: 592   score: 192.0  q_value: [289.9605528]   memory length: 7500\n",
      "episode: 593   score: 166.0  q_value: [290.75625236]   memory length: 7500\n",
      "episode: 594   score: 133.0  q_value: [291.90559763]   memory length: 7500\n",
      "episode: 595   score: 116.0  q_value: [292.59250157]   memory length: 7500\n",
      "episode: 596   score: 137.0  q_value: [292.54812705]   memory length: 7500\n",
      "episode: 597   score: 139.0  q_value: [293.37087612]   memory length: 7500\n",
      "episode: 598   score: 126.0  q_value: [292.50404008]   memory length: 7500\n",
      "episode: 599   score: 130.0  q_value: [291.53945532]   memory length: 7500\n",
      "episode: 600   score: 141.0  q_value: [291.1846357]   memory length: 7500\n",
      "episode: 601   score: 152.0  q_value: [291.26118917]   memory length: 7500\n",
      "episode: 602   score: 150.0  q_value: [290.30385135]   memory length: 7500\n",
      "episode: 603   score: 134.0  q_value: [290.39324653]   memory length: 7500\n",
      "episode: 604   score: 149.0  q_value: [291.47924487]   memory length: 7500\n",
      "episode: 605   score: 131.0  q_value: [293.23642954]   memory length: 7500\n",
      "episode: 606   score: 129.0  q_value: [293.45211182]   memory length: 7500\n",
      "episode: 607   score: 123.0  q_value: [293.30701654]   memory length: 7500\n",
      "episode: 608   score: 127.0  q_value: [294.89162988]   memory length: 7500\n",
      "episode: 609   score: 128.0  q_value: [296.23193251]   memory length: 7500\n",
      "episode: 610   score: 128.0  q_value: [297.9373167]   memory length: 7500\n",
      "episode: 611   score: 161.0  q_value: [297.53670094]   memory length: 7500\n",
      "episode: 612   score: 139.0  q_value: [297.1965752]   memory length: 7500\n",
      "episode: 613   score: 152.0  q_value: [296.17904829]   memory length: 7500\n",
      "episode: 614   score: 200.0  q_value: [296.72519994]   memory length: 7500\n",
      "episode: 615   score: 161.0  q_value: [296.5280756]   memory length: 7500\n",
      "episode: 616   score: 189.0  q_value: [296.10475561]   memory length: 7500\n",
      "episode: 617   score: 200.0  q_value: [296.26905991]   memory length: 7500\n",
      "episode: 618   score: 200.0  q_value: [295.69462634]   memory length: 7500\n",
      "episode: 619   score: 200.0  q_value: [293.98689139]   memory length: 7500\n",
      "episode: 620   score: 200.0  q_value: [294.15668467]   memory length: 7500\n",
      "episode: 621   score: 200.0  q_value: [292.73251416]   memory length: 7500\n",
      "episode: 622   score: 200.0  q_value: [291.8753067]   memory length: 7500\n",
      "episode: 623   score: 200.0  q_value: [289.74857676]   memory length: 7500\n",
      "episode: 624   score: 200.0  q_value: [287.12817599]   memory length: 7500\n",
      "episode: 625   score: 200.0  q_value: [286.72200381]   memory length: 7500\n",
      "episode: 626   score: 149.0  q_value: [286.69770975]   memory length: 7500\n",
      "episode: 627   score: 200.0  q_value: [286.3819828]   memory length: 7500\n",
      "episode: 628   score: 112.0  q_value: [285.04475073]   memory length: 7500\n",
      "episode: 629   score: 47.0  q_value: [283.45822593]   memory length: 7500\n",
      "episode: 630   score: 10.0  q_value: [284.42260263]   memory length: 7500\n",
      "episode: 631   score: 14.0  q_value: [285.17685871]   memory length: 7500\n",
      "episode: 632   score: 17.0  q_value: [285.16255008]   memory length: 7500\n",
      "episode: 633   score: 28.0  q_value: [284.66583355]   memory length: 7500\n",
      "episode: 634   score: 35.0  q_value: [285.52254847]   memory length: 7500\n",
      "episode: 635   score: 112.0  q_value: [285.20683726]   memory length: 7500\n",
      "episode: 636   score: 125.0  q_value: [283.86007835]   memory length: 7500\n",
      "episode: 637   score: 41.0  q_value: [285.69415258]   memory length: 7500\n",
      "episode: 638   score: 200.0  q_value: [289.38047022]   memory length: 7500\n",
      "episode: 639   score: 200.0  q_value: [290.66864609]   memory length: 7500\n",
      "episode: 640   score: 200.0  q_value: [289.83780412]   memory length: 7500\n",
      "episode: 641   score: 200.0  q_value: [290.08123358]   memory length: 7500\n",
      "episode: 642   score: 200.0  q_value: [292.68758868]   memory length: 7500\n",
      "episode: 643   score: 200.0  q_value: [296.63521459]   memory length: 7500\n",
      "episode: 644   score: 200.0  q_value: [301.18958955]   memory length: 7500\n",
      "episode: 645   score: 200.0  q_value: [302.94289722]   memory length: 7500\n",
      "episode: 646   score: 200.0  q_value: [307.66348484]   memory length: 7500\n",
      "episode: 647   score: 200.0  q_value: [308.43464585]   memory length: 7500\n",
      "episode: 648   score: 200.0  q_value: [310.87445437]   memory length: 7500\n",
      "episode: 649   score: 200.0  q_value: [315.25868578]   memory length: 7500\n",
      "episode: 650   score: 200.0  q_value: [315.49693919]   memory length: 7500\n",
      "episode: 651   score: 200.0  q_value: [320.2269151]   memory length: 7500\n",
      "episode: 652   score: 200.0  q_value: [322.48180047]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 653   score: 200.0  q_value: [323.47945586]   memory length: 7500\n",
      "episode: 654   score: 200.0  q_value: [322.27674516]   memory length: 7500\n",
      "episode: 655   score: 200.0  q_value: [322.18953846]   memory length: 7500\n",
      "episode: 656   score: 200.0  q_value: [320.53065494]   memory length: 7500\n",
      "episode: 657   score: 200.0  q_value: [322.00263021]   memory length: 7500\n",
      "episode: 658   score: 172.0  q_value: [323.38285415]   memory length: 7500\n",
      "episode: 659   score: 200.0  q_value: [324.0745003]   memory length: 7500\n",
      "episode: 660   score: 200.0  q_value: [324.35992305]   memory length: 7500\n",
      "episode: 661   score: 200.0  q_value: [324.30729501]   memory length: 7500\n",
      "episode: 662   score: 164.0  q_value: [322.26156218]   memory length: 7500\n",
      "episode: 663   score: 197.0  q_value: [325.24540742]   memory length: 7500\n",
      "episode: 664   score: 200.0  q_value: [324.24208252]   memory length: 7500\n",
      "episode: 665   score: 200.0  q_value: [324.09918092]   memory length: 7500\n",
      "episode: 666   score: 200.0  q_value: [325.13323824]   memory length: 7500\n",
      "episode: 667   score: 200.0  q_value: [328.87507966]   memory length: 7500\n",
      "episode: 668   score: 200.0  q_value: [329.42896703]   memory length: 7500\n",
      "episode: 669   score: 200.0  q_value: [332.44800005]   memory length: 7500\n",
      "episode: 670   score: 200.0  q_value: [332.10246401]   memory length: 7500\n",
      "episode: 671   score: 200.0  q_value: [331.56913476]   memory length: 7500\n",
      "episode: 672   score: 200.0  q_value: [333.07535818]   memory length: 7500\n",
      "episode: 673   score: 200.0  q_value: [331.7561401]   memory length: 7500\n",
      "episode: 674   score: 200.0  q_value: [329.15794826]   memory length: 7500\n",
      "episode: 675   score: 200.0  q_value: [330.2500898]   memory length: 7500\n",
      "episode: 676   score: 200.0  q_value: [329.53271044]   memory length: 7500\n",
      "episode: 677   score: 200.0  q_value: [328.09261056]   memory length: 7500\n",
      "episode: 678   score: 200.0  q_value: [327.4821155]   memory length: 7500\n",
      "episode: 679   score: 200.0  q_value: [326.15893744]   memory length: 7500\n",
      "episode: 680   score: 200.0  q_value: [327.45275118]   memory length: 7500\n",
      "episode: 681   score: 200.0  q_value: [325.76542194]   memory length: 7500\n",
      "episode: 682   score: 200.0  q_value: [326.477479]   memory length: 7500\n",
      "episode: 683   score: 200.0  q_value: [324.94842346]   memory length: 7500\n",
      "episode: 684   score: 200.0  q_value: [324.30431587]   memory length: 7500\n",
      "episode: 685   score: 200.0  q_value: [323.54619057]   memory length: 7500\n",
      "episode: 686   score: 200.0  q_value: [323.98122167]   memory length: 7500\n",
      "episode: 687   score: 200.0  q_value: [324.20854696]   memory length: 7500\n",
      "episode: 688   score: 200.0  q_value: [323.91063143]   memory length: 7500\n",
      "episode: 689   score: 200.0  q_value: [323.78556768]   memory length: 7500\n",
      "episode: 690   score: 200.0  q_value: [323.25872759]   memory length: 7500\n",
      "episode: 691   score: 200.0  q_value: [324.42615701]   memory length: 7500\n",
      "episode: 692   score: 200.0  q_value: [323.4179107]   memory length: 7500\n",
      "episode: 693   score: 200.0  q_value: [323.3019757]   memory length: 7500\n",
      "episode: 694   score: 200.0  q_value: [322.70099384]   memory length: 7500\n",
      "episode: 695   score: 200.0  q_value: [322.99675337]   memory length: 7500\n",
      "episode: 696   score: 200.0  q_value: [322.75468116]   memory length: 7500\n",
      "episode: 697   score: 200.0  q_value: [321.39009766]   memory length: 7500\n",
      "episode: 698   score: 200.0  q_value: [321.30901073]   memory length: 7500\n",
      "episode: 699   score: 200.0  q_value: [320.2618294]   memory length: 7500\n",
      "episode: 700   score: 200.0  q_value: [319.12834118]   memory length: 7500\n",
      "episode: 701   score: 200.0  q_value: [318.4086801]   memory length: 7500\n",
      "episode: 702   score: 200.0  q_value: [317.65326231]   memory length: 7500\n",
      "episode: 703   score: 200.0  q_value: [315.9520359]   memory length: 7500\n",
      "episode: 704   score: 200.0  q_value: [315.81293631]   memory length: 7500\n",
      "episode: 705   score: 200.0  q_value: [314.98849856]   memory length: 7500\n",
      "episode: 706   score: 200.0  q_value: [314.36682351]   memory length: 7500\n",
      "episode: 707   score: 200.0  q_value: [313.7397156]   memory length: 7500\n",
      "episode: 708   score: 200.0  q_value: [312.24013122]   memory length: 7500\n",
      "episode: 709   score: 200.0  q_value: [312.11555429]   memory length: 7500\n",
      "episode: 710   score: 200.0  q_value: [310.42931175]   memory length: 7500\n",
      "episode: 711   score: 200.0  q_value: [309.64566916]   memory length: 7500\n",
      "episode: 712   score: 200.0  q_value: [309.17560371]   memory length: 7500\n",
      "episode: 713   score: 200.0  q_value: [309.1514657]   memory length: 7500\n",
      "episode: 714   score: 200.0  q_value: [308.46843742]   memory length: 7500\n",
      "episode: 715   score: 200.0  q_value: [309.19583112]   memory length: 7500\n",
      "episode: 716   score: 200.0  q_value: [309.9294215]   memory length: 7500\n",
      "episode: 717   score: 200.0  q_value: [309.94280851]   memory length: 7500\n",
      "episode: 718   score: 200.0  q_value: [310.18216632]   memory length: 7500\n",
      "episode: 719   score: 200.0  q_value: [310.38504515]   memory length: 7500\n",
      "episode: 720   score: 200.0  q_value: [308.63244146]   memory length: 7500\n",
      "episode: 721   score: 200.0  q_value: [307.76854846]   memory length: 7500\n",
      "episode: 722   score: 200.0  q_value: [308.4211013]   memory length: 7500\n",
      "episode: 723   score: 200.0  q_value: [307.84988382]   memory length: 7500\n",
      "episode: 724   score: 200.0  q_value: [305.94425111]   memory length: 7500\n",
      "episode: 725   score: 200.0  q_value: [305.57615826]   memory length: 7500\n",
      "episode: 726   score: 200.0  q_value: [304.95721537]   memory length: 7500\n",
      "episode: 727   score: 200.0  q_value: [305.2736816]   memory length: 7500\n",
      "episode: 728   score: 200.0  q_value: [305.17371386]   memory length: 7500\n",
      "episode: 729   score: 200.0  q_value: [303.99613247]   memory length: 7500\n",
      "episode: 730   score: 200.0  q_value: [303.16644949]   memory length: 7500\n",
      "episode: 731   score: 200.0  q_value: [303.38761794]   memory length: 7500\n",
      "episode: 732   score: 200.0  q_value: [301.41931757]   memory length: 7500\n",
      "episode: 733   score: 200.0  q_value: [301.33550144]   memory length: 7500\n",
      "episode: 734   score: 200.0  q_value: [301.72604143]   memory length: 7500\n",
      "solved after 634 episodes\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabarishsridhar/Anaconda3/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 1000 #Maximum number of episodes 1000\n",
    "\n",
    "#DQN Agent for the Cartpole\n",
    "#Q function approximation with NN, experience replay, and target network\n",
    "class DQNAgent:\n",
    "    #Constructor for the agent (invoked when DQN is first called in main)\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.check_solve = True\t#If True, stop if you satisfy solution condition\n",
    "        self.render = True       #If you want to see Cartpole learning, then change to True\n",
    "\n",
    "        #Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "       # Modify here\n",
    "\n",
    "        #Set hyper parameters for the DQN. Do not adjust those labeled as Fixed.\n",
    "        self.discount_factor = 0.995#0.95\n",
    "        self.learning_rate = 0.005#0.005\n",
    "        self.epsilon = 0.02 #Fixed\n",
    "        self.batch_size = 32 #Fixed\n",
    "        self.memory_size = 7500#1000\n",
    "        self.train_start = 1000 #Fixed\n",
    "        self.target_update_frequency = 1#1\n",
    "\n",
    "        #Number of test states for Q value plots\n",
    "        self.test_state_no = 10000\n",
    "\n",
    "        #Create memory buffer using deque\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "\n",
    "        #Create main network and target network (using build_model defined below)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        #Initialize target network\n",
    "        self.update_target_model()\n",
    "\n",
    "    #Approximate Q function using Neural Network\n",
    "    #State is the input and the Q Values are the output.\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Edit the Neural Network model here\n",
    "        #Tip: Consult https://keras.io/getting-started/sequential-model-guide/\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform')) # 16\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "    #After some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your e-greedy policy code here\n",
    "        #Tip 1: Use the random package to generate a random action.\n",
    "        #Tip 2: Use keras.model.predict() to compute Q-values from the state.\n",
    "        \n",
    "        ''' So E Greedy is like, you take a random action with proabability epsilon, other with 1-e\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        q_val = self.model.predict(state)\n",
    "        #print(\"Something is fishy!\")\n",
    "        #print(q_val)\n",
    "        best_action = np.argmax(q_val);\n",
    "        #print(\" I choose \",best_action)\n",
    "        rand_act = random.randrange(self.action_size);\n",
    "        #print(\" The random value might be \", rand_act);\n",
    "        \n",
    "        \n",
    "        return np.random.choice([rand_act,best_action],p=[self.epsilon,1-self.epsilon]);\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "    #Save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #Add sample to the end of the list\n",
    "\n",
    "    #Sample <s,a,r,s'> from replay memory\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start: #Do not train if not enough memory\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory)) #Train on at most as many samples as you have in memory\n",
    "        mini_batch = random.sample(self.memory, batch_size) #Uniformly sample the memory buffer\n",
    "        #Preallocate network and target network input matrices.\n",
    "        update_input = np.zeros((batch_size, self.state_size)) #batch_size by state_size two-dimensional array (not matrix!)\n",
    "        update_target = np.zeros((batch_size, self.state_size)) #Same as above, but used for the target network\n",
    "        action, reward, done = [], [], [] #Empty arrays that will grow dynamically\n",
    "        y_batch = []#I Created this array\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #Allocate s(i) to the network input array from iteration i in the batch\n",
    "            action.append(mini_batch[i][1]) #Store a(i)\n",
    "            reward.append(mini_batch[i][2]) #Store r(i)\n",
    "            update_target[i] = mini_batch[i][3] #Allocate s'(i) for the target network array from iteration i in the batch\n",
    "            done.append(mini_batch[i][4])  #Store done(i)\n",
    "\n",
    "        target = self.model.predict(update_input) #Generate target values for training the inner loop network using the network model\n",
    "        target_val = self.target_model.predict(update_target) #Generate the target values for training the outer loop target network\n",
    "        #print(\"Target :\",target,\"Target_val :\",target_val)            \n",
    "        \n",
    "       # Added by me \n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size): # Setting yj and gradient descent  line 12-13\n",
    "        # if terminal only equals reward\n",
    "            if mini_batch[i][4]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] =  reward[i]+ self.discount_factor * np.max(target_val[i])\n",
    "            \n",
    "        #print(\"Q values in target\",target)\n",
    "        #Q Learning: get maximum Q value at s' from target network\n",
    "###############################################################################\n",
    "###########################x####################################################\n",
    "        #Insert your Q-learning code here\n",
    "        #Tip 1: Observe that the Q-values are stored in the variable target\n",
    "        #Tip 2: What is the Q-value of the action taken at the last state of the episode?\n",
    "        # print(target)\n",
    "        \n",
    "        \n",
    "#         for i in range(self.batch_size): #For every batch\n",
    "#             if mini_batch[i][4]:\n",
    "#                target[i][action[i]] = 1\n",
    "#             target[i][action[i]] += self.learning_rate *(reward[i] + (self.discount_factor * max(target[i]))- target[i][action[i]]) #random.randint(0,1)\n",
    "# ###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "        #Train the inner loop network\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        return\n",
    "    #Plots the score per episode as well as the maximum q value per episode, averaged over precollected states.\n",
    "    def plot_data(self, episodes, scores, max_q_mean):\n",
    "        pylab.figure(0)\n",
    "        pylab.plot(episodes, max_q_mean, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Average Q Value\")\n",
    "        pylab.savefig(\"qvalues.png\")\n",
    "\n",
    "        pylab.figure(1)\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Score\")\n",
    "        pylab.savefig(\"scores.png\")\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #For CartPole-v0, maximum episode length is 200\n",
    "    env = gym.make('CartPole-v0') #Generate Cartpole-v0 environment object from the gym library\n",
    "    #Get state and action sizes from the environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    #Create agent, see the DQNAgent __init__ method for details\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    #Collect test states for plotting Q values using uniform random policy\n",
    "    test_states = np.zeros((agent.test_state_no, state_size))\n",
    "    max_q = np.zeros((EPISODES, agent.test_state_no))\n",
    "    max_q_mean = np.zeros((EPISODES,1))\n",
    "\n",
    "    done = True\n",
    "    for i in range(agent.test_state_no):\n",
    "        if done:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "        else:\n",
    "            action = random.randrange(action_size)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "            state = next_state\n",
    "\n",
    "    scores, episodes = [], [] #Create dynamically growing score and episode counters\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset() #Initialize/reset the environment\n",
    "        state = np.reshape(state, [1, state_size]) #Reshape state so that to a 1 by state_size two-dimensional array ie. [x_1,x_2] to [[x_1,x_2]]\n",
    "        #Compute Q values for plotting\n",
    "        tmp = agent.model.predict(test_states)\n",
    "        max_q[e][:] = np.max(tmp, axis=1)\n",
    "        max_q_mean[e] = np.mean(max_q[e][:])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render() #Show cartpole animation\n",
    "\n",
    "            #Get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state) # Selecting the action by epsilon greedy -sab #Line 6 and 7\n",
    "            next_state, reward, done, info = env.step(action) # Executing the action  - sab # Line 8\n",
    "            next_state = np.reshape(next_state, [1, state_size]) #Reshape next_state similarly to state # maybe line 9\n",
    "\n",
    "            #Save sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)  # this looks like the storing transition in D - sab # Line 10\n",
    "            #Training step\n",
    "            agent.train_model()\n",
    "            score += reward #Store episodic reward\n",
    "            state = next_state #Propagate state\n",
    "\n",
    "            if done:\n",
    "                #At the end of very episode, update the target network\n",
    "                if e % agent.target_update_frequency == 0:\n",
    "                    agent.update_target_model()\n",
    "                #Plot the play time for every episode\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score,\" q_value:\", max_q_mean[e],\"  memory length:\",\n",
    "                      len(agent.memory))\n",
    "\n",
    "                # if the mean of scores of last 100 episodes is bigger than 195\n",
    "                # stop training\n",
    "                if agent.check_solve:\n",
    "                    if np.mean(scores[-min(100, len(scores)):]) >= 195:\n",
    "                        print(\"solved after\", e-100, \"episodes\")\n",
    "                        agent.plot_data(episodes,scores,max_q_mean[:e+1])\n",
    "                        sys.exit()\n",
    "    agent.plot_data(episodes,scores,max_q_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (735,) and (1000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0557b34a6e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_q_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-57646a7a3c36>\u001b[0m in \u001b[0;36mplot_data\u001b[0;34m(self, episodes, scores, max_q_mean)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episodes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Q Value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m     return gca().plot(\n\u001b[1;32m   2794\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2795\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1664\u001b[0m         \"\"\"\n\u001b[1;32m   1665\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (735,) and (1000, 1)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.plot_data(episodes,scores,max_q_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
