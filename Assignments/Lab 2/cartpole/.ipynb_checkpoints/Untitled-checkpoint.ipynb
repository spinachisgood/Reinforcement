{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0   score: 21.0  q_value: [0.94553636]   memory length: 21\n",
      "episode: 1   score: 8.0  q_value: [0.94553636]   memory length: 29\n",
      "episode: 2   score: 10.0  q_value: [0.94553636]   memory length: 39\n",
      "episode: 3   score: 13.0  q_value: [0.94553636]   memory length: 52\n",
      "episode: 4   score: 9.0  q_value: [0.94553636]   memory length: 61\n",
      "episode: 5   score: 10.0  q_value: [0.94553636]   memory length: 71\n",
      "episode: 6   score: 10.0  q_value: [0.94553636]   memory length: 81\n",
      "episode: 7   score: 21.0  q_value: [0.94553636]   memory length: 102\n",
      "episode: 8   score: 8.0  q_value: [0.94553636]   memory length: 110\n",
      "episode: 9   score: 10.0  q_value: [0.94553636]   memory length: 120\n",
      "episode: 10   score: 16.0  q_value: [0.94553636]   memory length: 136\n",
      "episode: 11   score: 12.0  q_value: [0.94553636]   memory length: 148\n",
      "episode: 12   score: 13.0  q_value: [0.94553636]   memory length: 161\n",
      "episode: 13   score: 9.0  q_value: [0.94553636]   memory length: 170\n",
      "episode: 14   score: 10.0  q_value: [0.94553636]   memory length: 180\n",
      "episode: 15   score: 19.0  q_value: [0.94553636]   memory length: 199\n",
      "episode: 16   score: 10.0  q_value: [0.94553636]   memory length: 209\n",
      "episode: 17   score: 10.0  q_value: [0.94553636]   memory length: 219\n",
      "episode: 18   score: 10.0  q_value: [0.94553636]   memory length: 229\n",
      "episode: 19   score: 15.0  q_value: [0.94553636]   memory length: 244\n",
      "episode: 20   score: 10.0  q_value: [0.94553636]   memory length: 254\n",
      "episode: 21   score: 10.0  q_value: [0.94553636]   memory length: 264\n",
      "episode: 22   score: 10.0  q_value: [0.94553636]   memory length: 274\n",
      "episode: 23   score: 11.0  q_value: [0.94553636]   memory length: 285\n",
      "episode: 24   score: 10.0  q_value: [0.94553636]   memory length: 295\n",
      "episode: 25   score: 10.0  q_value: [0.94553636]   memory length: 305\n",
      "episode: 26   score: 11.0  q_value: [0.94553636]   memory length: 316\n",
      "episode: 27   score: 10.0  q_value: [0.94553636]   memory length: 326\n",
      "episode: 28   score: 11.0  q_value: [0.94553636]   memory length: 337\n",
      "episode: 29   score: 12.0  q_value: [0.94553636]   memory length: 349\n",
      "episode: 30   score: 9.0  q_value: [0.94553636]   memory length: 358\n",
      "episode: 31   score: 12.0  q_value: [0.94553636]   memory length: 370\n",
      "episode: 32   score: 10.0  q_value: [0.94553636]   memory length: 380\n",
      "episode: 33   score: 15.0  q_value: [0.94553636]   memory length: 395\n",
      "episode: 34   score: 12.0  q_value: [0.94553636]   memory length: 407\n",
      "episode: 35   score: 11.0  q_value: [0.94553636]   memory length: 418\n",
      "episode: 36   score: 9.0  q_value: [0.94553636]   memory length: 427\n",
      "episode: 37   score: 9.0  q_value: [0.94553636]   memory length: 436\n",
      "episode: 38   score: 9.0  q_value: [0.94553636]   memory length: 445\n",
      "episode: 39   score: 10.0  q_value: [0.94553636]   memory length: 455\n",
      "episode: 40   score: 10.0  q_value: [0.94553636]   memory length: 465\n",
      "episode: 41   score: 10.0  q_value: [0.94553636]   memory length: 475\n",
      "episode: 42   score: 12.0  q_value: [0.94553636]   memory length: 487\n",
      "episode: 43   score: 9.0  q_value: [0.94553636]   memory length: 496\n",
      "episode: 44   score: 11.0  q_value: [0.94553636]   memory length: 507\n",
      "episode: 45   score: 10.0  q_value: [0.94553636]   memory length: 517\n",
      "episode: 46   score: 10.0  q_value: [0.94553636]   memory length: 527\n",
      "episode: 47   score: 11.0  q_value: [0.94553636]   memory length: 538\n",
      "episode: 48   score: 10.0  q_value: [0.94553636]   memory length: 548\n",
      "episode: 49   score: 16.0  q_value: [0.94553636]   memory length: 564\n",
      "episode: 50   score: 10.0  q_value: [0.94553636]   memory length: 574\n",
      "episode: 51   score: 10.0  q_value: [0.94553636]   memory length: 584\n",
      "episode: 52   score: 11.0  q_value: [0.94553636]   memory length: 595\n",
      "episode: 53   score: 9.0  q_value: [0.94553636]   memory length: 604\n",
      "episode: 54   score: 10.0  q_value: [0.94553636]   memory length: 614\n",
      "episode: 55   score: 10.0  q_value: [0.94553636]   memory length: 624\n",
      "episode: 56   score: 9.0  q_value: [0.94553636]   memory length: 633\n",
      "episode: 57   score: 10.0  q_value: [0.94553636]   memory length: 643\n",
      "episode: 58   score: 16.0  q_value: [0.94553636]   memory length: 659\n",
      "episode: 59   score: 10.0  q_value: [0.94553636]   memory length: 669\n",
      "episode: 60   score: 11.0  q_value: [0.94553636]   memory length: 680\n",
      "episode: 61   score: 9.0  q_value: [0.94553636]   memory length: 689\n",
      "episode: 62   score: 11.0  q_value: [0.94553636]   memory length: 700\n",
      "episode: 63   score: 9.0  q_value: [0.94553636]   memory length: 709\n",
      "episode: 64   score: 11.0  q_value: [0.94553636]   memory length: 720\n",
      "episode: 65   score: 10.0  q_value: [0.94553636]   memory length: 730\n",
      "episode: 66   score: 10.0  q_value: [0.94553636]   memory length: 740\n",
      "episode: 67   score: 10.0  q_value: [0.94553636]   memory length: 750\n",
      "episode: 68   score: 10.0  q_value: [0.94553636]   memory length: 760\n",
      "episode: 69   score: 9.0  q_value: [0.94553636]   memory length: 769\n",
      "episode: 70   score: 10.0  q_value: [0.94553636]   memory length: 779\n",
      "episode: 71   score: 24.0  q_value: [0.94553636]   memory length: 803\n",
      "episode: 72   score: 13.0  q_value: [0.94553636]   memory length: 816\n",
      "episode: 73   score: 10.0  q_value: [0.94553636]   memory length: 826\n",
      "episode: 74   score: 9.0  q_value: [0.94553636]   memory length: 835\n",
      "episode: 75   score: 9.0  q_value: [0.94553636]   memory length: 844\n",
      "episode: 76   score: 9.0  q_value: [0.94553636]   memory length: 853\n",
      "episode: 77   score: 10.0  q_value: [0.94553636]   memory length: 863\n",
      "episode: 78   score: 10.0  q_value: [0.94553636]   memory length: 873\n",
      "episode: 79   score: 10.0  q_value: [0.94553636]   memory length: 883\n",
      "episode: 80   score: 10.0  q_value: [0.94553636]   memory length: 893\n",
      "episode: 81   score: 10.0  q_value: [0.94553636]   memory length: 903\n",
      "episode: 82   score: 10.0  q_value: [0.94553636]   memory length: 913\n",
      "episode: 83   score: 10.0  q_value: [0.94553636]   memory length: 923\n",
      "episode: 84   score: 10.0  q_value: [0.94553636]   memory length: 933\n",
      "episode: 85   score: 10.0  q_value: [0.94553636]   memory length: 943\n",
      "episode: 86   score: 10.0  q_value: [0.94553636]   memory length: 953\n",
      "episode: 87   score: 10.0  q_value: [0.94553636]   memory length: 963\n",
      "episode: 88   score: 10.0  q_value: [0.94553636]   memory length: 973\n",
      "episode: 89   score: 11.0  q_value: [0.94553636]   memory length: 984\n",
      "episode: 90   score: 10.0  q_value: [0.94553636]   memory length: 994\n",
      "episode: 91   score: 10.0  q_value: [0.94553636]   memory length: 1004\n",
      "episode: 92   score: 10.0  q_value: [1.44024213]   memory length: 1014\n",
      "episode: 93   score: 25.0  q_value: [1.90815758]   memory length: 1039\n",
      "episode: 94   score: 10.0  q_value: [1.85297225]   memory length: 1049\n",
      "episode: 95   score: 11.0  q_value: [1.98328701]   memory length: 1060\n",
      "episode: 96   score: 10.0  q_value: [2.21782947]   memory length: 1070\n",
      "episode: 97   score: 10.0  q_value: [2.9405798]   memory length: 1080\n",
      "episode: 98   score: 11.0  q_value: [3.67270027]   memory length: 1091\n",
      "episode: 99   score: 10.0  q_value: [3.90881073]   memory length: 1101\n",
      "episode: 100   score: 10.0  q_value: [3.76984781]   memory length: 1111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 101   score: 10.0  q_value: [3.83945466]   memory length: 1121\n",
      "episode: 102   score: 11.0  q_value: [4.38254139]   memory length: 1132\n",
      "episode: 103   score: 11.0  q_value: [5.37587245]   memory length: 1143\n",
      "episode: 104   score: 10.0  q_value: [5.40654903]   memory length: 1153\n",
      "episode: 105   score: 10.0  q_value: [5.29364185]   memory length: 1163\n",
      "episode: 106   score: 9.0  q_value: [5.17582597]   memory length: 1172\n",
      "episode: 107   score: 13.0  q_value: [5.47766879]   memory length: 1185\n",
      "episode: 108   score: 9.0  q_value: [6.46097835]   memory length: 1194\n",
      "episode: 109   score: 11.0  q_value: [6.32967842]   memory length: 1205\n",
      "episode: 110   score: 9.0  q_value: [5.95176911]   memory length: 1214\n",
      "episode: 111   score: 9.0  q_value: [5.80222755]   memory length: 1223\n",
      "episode: 112   score: 10.0  q_value: [6.50407125]   memory length: 1233\n",
      "episode: 113   score: 9.0  q_value: [7.24360596]   memory length: 1242\n",
      "episode: 114   score: 9.0  q_value: [7.2809801]   memory length: 1251\n",
      "episode: 115   score: 11.0  q_value: [6.91508998]   memory length: 1262\n",
      "episode: 116   score: 10.0  q_value: [6.88666219]   memory length: 1272\n",
      "episode: 117   score: 11.0  q_value: [7.89088756]   memory length: 1283\n",
      "episode: 118   score: 10.0  q_value: [8.455559]   memory length: 1293\n",
      "episode: 119   score: 10.0  q_value: [8.01554401]   memory length: 1303\n",
      "episode: 120   score: 12.0  q_value: [7.73202697]   memory length: 1315\n",
      "episode: 121   score: 10.0  q_value: [7.93683187]   memory length: 1325\n",
      "episode: 122   score: 12.0  q_value: [8.69592273]   memory length: 1337\n",
      "episode: 123   score: 10.0  q_value: [9.21751936]   memory length: 1347\n",
      "episode: 124   score: 33.0  q_value: [8.81290119]   memory length: 1380\n",
      "episode: 125   score: 20.0  q_value: [9.01822382]   memory length: 1400\n",
      "episode: 126   score: 23.0  q_value: [8.91626278]   memory length: 1423\n",
      "episode: 127   score: 10.0  q_value: [10.39701513]   memory length: 1433\n",
      "episode: 128   score: 9.0  q_value: [10.06508767]   memory length: 1442\n",
      "episode: 129   score: 16.0  q_value: [9.7668767]   memory length: 1458\n",
      "episode: 130   score: 14.0  q_value: [9.45881687]   memory length: 1472\n",
      "episode: 131   score: 22.0  q_value: [9.51044677]   memory length: 1494\n",
      "episode: 132   score: 16.0  q_value: [10.83943537]   memory length: 1510\n",
      "episode: 133   score: 15.0  q_value: [10.33623272]   memory length: 1525\n",
      "episode: 134   score: 10.0  q_value: [11.42301756]   memory length: 1535\n",
      "episode: 135   score: 10.0  q_value: [10.84185522]   memory length: 1545\n",
      "episode: 136   score: 53.0  q_value: [10.01220424]   memory length: 1598\n",
      "episode: 137   score: 11.0  q_value: [10.81119507]   memory length: 1609\n",
      "episode: 138   score: 17.0  q_value: [10.76941656]   memory length: 1626\n",
      "episode: 139   score: 9.0  q_value: [11.35064137]   memory length: 1635\n",
      "episode: 140   score: 8.0  q_value: [11.6232733]   memory length: 1643\n",
      "episode: 141   score: 10.0  q_value: [11.09926201]   memory length: 1653\n",
      "episode: 142   score: 11.0  q_value: [11.75891481]   memory length: 1664\n",
      "episode: 143   score: 21.0  q_value: [11.87096613]   memory length: 1685\n",
      "episode: 144   score: 9.0  q_value: [11.44091683]   memory length: 1694\n",
      "episode: 145   score: 11.0  q_value: [11.92082741]   memory length: 1705\n",
      "episode: 146   score: 16.0  q_value: [11.59594788]   memory length: 1721\n",
      "episode: 147   score: 35.0  q_value: [12.24183445]   memory length: 1756\n",
      "episode: 148   score: 38.0  q_value: [12.34256836]   memory length: 1794\n",
      "episode: 149   score: 18.0  q_value: [12.46469054]   memory length: 1812\n",
      "episode: 150   score: 11.0  q_value: [12.34932965]   memory length: 1823\n",
      "episode: 151   score: 28.0  q_value: [12.25772971]   memory length: 1851\n",
      "episode: 152   score: 23.0  q_value: [12.2097028]   memory length: 1874\n",
      "episode: 153   score: 10.0  q_value: [12.45678689]   memory length: 1884\n",
      "episode: 154   score: 9.0  q_value: [13.12301361]   memory length: 1893\n",
      "episode: 155   score: 10.0  q_value: [13.09508625]   memory length: 1903\n",
      "episode: 156   score: 53.0  q_value: [12.72900692]   memory length: 1956\n",
      "episode: 157   score: 28.0  q_value: [13.16730568]   memory length: 1984\n",
      "episode: 158   score: 30.0  q_value: [13.25873671]   memory length: 2014\n",
      "episode: 159   score: 45.0  q_value: [13.66472517]   memory length: 2059\n",
      "episode: 160   score: 48.0  q_value: [13.42170337]   memory length: 2107\n",
      "episode: 161   score: 68.0  q_value: [13.32331369]   memory length: 2175\n",
      "episode: 162   score: 78.0  q_value: [14.22255115]   memory length: 2253\n",
      "episode: 163   score: 62.0  q_value: [13.95629754]   memory length: 2315\n",
      "episode: 164   score: 63.0  q_value: [14.21139066]   memory length: 2378\n",
      "episode: 165   score: 65.0  q_value: [14.34128533]   memory length: 2443\n",
      "episode: 166   score: 132.0  q_value: [14.32565921]   memory length: 2575\n",
      "episode: 167   score: 58.0  q_value: [14.8832732]   memory length: 2633\n",
      "episode: 168   score: 69.0  q_value: [15.01708978]   memory length: 2702\n",
      "episode: 169   score: 84.0  q_value: [15.38736775]   memory length: 2786\n",
      "episode: 170   score: 86.0  q_value: [14.8215579]   memory length: 2872\n",
      "episode: 171   score: 66.0  q_value: [15.01420722]   memory length: 2938\n",
      "episode: 172   score: 133.0  q_value: [16.09200246]   memory length: 3071\n",
      "episode: 173   score: 72.0  q_value: [15.9613036]   memory length: 3143\n",
      "episode: 174   score: 69.0  q_value: [15.99184709]   memory length: 3212\n",
      "episode: 175   score: 66.0  q_value: [15.73136122]   memory length: 3278\n",
      "episode: 176   score: 122.0  q_value: [16.05726623]   memory length: 3400\n",
      "episode: 177   score: 110.0  q_value: [17.08431036]   memory length: 3510\n",
      "episode: 178   score: 74.0  q_value: [16.76705882]   memory length: 3584\n",
      "episode: 179   score: 100.0  q_value: [16.60640205]   memory length: 3684\n",
      "episode: 180   score: 113.0  q_value: [17.04696218]   memory length: 3797\n",
      "episode: 181   score: 100.0  q_value: [16.97937818]   memory length: 3897\n",
      "episode: 182   score: 107.0  q_value: [18.09476879]   memory length: 4004\n",
      "episode: 183   score: 95.0  q_value: [18.04201611]   memory length: 4099\n",
      "episode: 184   score: 98.0  q_value: [17.91778775]   memory length: 4197\n",
      "episode: 185   score: 111.0  q_value: [17.84343882]   memory length: 4308\n",
      "episode: 186   score: 85.0  q_value: [17.3266694]   memory length: 4393\n",
      "episode: 187   score: 85.0  q_value: [18.34049455]   memory length: 4478\n",
      "episode: 188   score: 97.0  q_value: [18.34437623]   memory length: 4575\n",
      "episode: 189   score: 118.0  q_value: [18.0247855]   memory length: 4693\n",
      "episode: 190   score: 86.0  q_value: [17.63458178]   memory length: 4779\n",
      "episode: 191   score: 113.0  q_value: [18.11964344]   memory length: 4892\n",
      "episode: 192   score: 99.0  q_value: [19.15254999]   memory length: 4991\n",
      "episode: 193   score: 92.0  q_value: [19.04660619]   memory length: 5083\n",
      "episode: 194   score: 136.0  q_value: [18.88839043]   memory length: 5219\n",
      "episode: 195   score: 96.0  q_value: [19.08366156]   memory length: 5315\n",
      "episode: 196   score: 114.0  q_value: [19.13681187]   memory length: 5429\n",
      "episode: 197   score: 143.0  q_value: [20.52858645]   memory length: 5572\n",
      "episode: 198   score: 113.0  q_value: [19.9752592]   memory length: 5685\n",
      "episode: 199   score: 136.0  q_value: [20.12579505]   memory length: 5821\n",
      "episode: 200   score: 137.0  q_value: [19.86380935]   memory length: 5958\n",
      "episode: 201   score: 125.0  q_value: [20.3646433]   memory length: 6083\n",
      "episode: 202   score: 91.0  q_value: [21.21181495]   memory length: 6174\n",
      "episode: 203   score: 94.0  q_value: [21.28530319]   memory length: 6268\n",
      "episode: 204   score: 106.0  q_value: [21.04899218]   memory length: 6374\n",
      "episode: 205   score: 106.0  q_value: [21.12292252]   memory length: 6480\n",
      "episode: 206   score: 96.0  q_value: [21.25829716]   memory length: 6576\n",
      "episode: 207   score: 104.0  q_value: [22.27888559]   memory length: 6680\n",
      "episode: 208   score: 151.0  q_value: [22.06938403]   memory length: 6831\n",
      "episode: 209   score: 92.0  q_value: [22.21662402]   memory length: 6923\n",
      "episode: 210   score: 137.0  q_value: [22.03895413]   memory length: 7060\n",
      "episode: 211   score: 98.0  q_value: [22.20623011]   memory length: 7158\n",
      "episode: 212   score: 116.0  q_value: [22.71458187]   memory length: 7274\n",
      "episode: 213   score: 138.0  q_value: [23.02233987]   memory length: 7412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 214   score: 124.0  q_value: [23.12402648]   memory length: 7500\n",
      "episode: 215   score: 118.0  q_value: [23.05599963]   memory length: 7500\n",
      "episode: 216   score: 119.0  q_value: [22.90012828]   memory length: 7500\n",
      "episode: 217   score: 116.0  q_value: [23.46976946]   memory length: 7500\n",
      "episode: 218   score: 122.0  q_value: [23.7168902]   memory length: 7500\n",
      "episode: 219   score: 112.0  q_value: [24.12942013]   memory length: 7500\n",
      "episode: 220   score: 108.0  q_value: [23.86017105]   memory length: 7500\n",
      "episode: 221   score: 109.0  q_value: [24.1674596]   memory length: 7500\n",
      "episode: 222   score: 113.0  q_value: [25.05809121]   memory length: 7500\n",
      "episode: 223   score: 108.0  q_value: [25.21885956]   memory length: 7500\n",
      "episode: 224   score: 112.0  q_value: [24.67435455]   memory length: 7500\n",
      "episode: 225   score: 107.0  q_value: [24.76977584]   memory length: 7500\n",
      "episode: 226   score: 142.0  q_value: [25.05402526]   memory length: 7500\n",
      "episode: 227   score: 126.0  q_value: [26.24537058]   memory length: 7500\n",
      "episode: 228   score: 130.0  q_value: [26.3171725]   memory length: 7500\n",
      "episode: 229   score: 128.0  q_value: [26.32163243]   memory length: 7500\n",
      "episode: 230   score: 121.0  q_value: [26.2963476]   memory length: 7500\n",
      "episode: 231   score: 117.0  q_value: [26.44927144]   memory length: 7500\n",
      "episode: 232   score: 112.0  q_value: [27.412152]   memory length: 7500\n",
      "episode: 233   score: 143.0  q_value: [27.62186384]   memory length: 7500\n",
      "episode: 234   score: 131.0  q_value: [27.40090312]   memory length: 7500\n",
      "episode: 235   score: 121.0  q_value: [27.31616647]   memory length: 7500\n",
      "episode: 236   score: 119.0  q_value: [27.43629305]   memory length: 7500\n",
      "episode: 237   score: 126.0  q_value: [28.48096081]   memory length: 7500\n",
      "episode: 238   score: 131.0  q_value: [28.51214047]   memory length: 7500\n",
      "episode: 239   score: 154.0  q_value: [28.67825504]   memory length: 7500\n",
      "episode: 240   score: 136.0  q_value: [28.7165725]   memory length: 7500\n",
      "episode: 241   score: 139.0  q_value: [28.9809547]   memory length: 7500\n",
      "episode: 242   score: 115.0  q_value: [29.68598633]   memory length: 7500\n",
      "episode: 243   score: 142.0  q_value: [30.0200574]   memory length: 7500\n",
      "episode: 244   score: 130.0  q_value: [29.763144]   memory length: 7500\n",
      "episode: 245   score: 136.0  q_value: [30.23172049]   memory length: 7500\n",
      "episode: 246   score: 126.0  q_value: [29.89349583]   memory length: 7500\n",
      "episode: 247   score: 128.0  q_value: [30.39826584]   memory length: 7500\n",
      "episode: 248   score: 148.0  q_value: [30.48701389]   memory length: 7500\n",
      "episode: 249   score: 142.0  q_value: [30.97298913]   memory length: 7500\n",
      "episode: 250   score: 136.0  q_value: [30.97472931]   memory length: 7500\n",
      "episode: 251   score: 119.0  q_value: [30.73114122]   memory length: 7500\n",
      "episode: 252   score: 111.0  q_value: [31.82845267]   memory length: 7500\n",
      "episode: 253   score: 117.0  q_value: [31.66867785]   memory length: 7500\n",
      "episode: 254   score: 108.0  q_value: [32.02655596]   memory length: 7500\n",
      "episode: 255   score: 146.0  q_value: [32.08241091]   memory length: 7500\n",
      "episode: 256   score: 140.0  q_value: [32.05136758]   memory length: 7500\n",
      "episode: 257   score: 137.0  q_value: [32.86972291]   memory length: 7500\n",
      "episode: 258   score: 117.0  q_value: [33.00942921]   memory length: 7500\n",
      "episode: 259   score: 135.0  q_value: [32.89555229]   memory length: 7500\n",
      "episode: 260   score: 145.0  q_value: [32.96254837]   memory length: 7500\n",
      "episode: 261   score: 150.0  q_value: [33.03979486]   memory length: 7500\n",
      "episode: 262   score: 130.0  q_value: [33.9172468]   memory length: 7500\n",
      "episode: 263   score: 125.0  q_value: [33.96156435]   memory length: 7500\n",
      "episode: 264   score: 145.0  q_value: [33.92158988]   memory length: 7500\n",
      "episode: 265   score: 138.0  q_value: [33.94275437]   memory length: 7500\n",
      "episode: 266   score: 156.0  q_value: [34.0845496]   memory length: 7500\n",
      "episode: 267   score: 149.0  q_value: [34.8363704]   memory length: 7500\n",
      "episode: 268   score: 124.0  q_value: [34.95200748]   memory length: 7500\n",
      "episode: 269   score: 141.0  q_value: [34.99513525]   memory length: 7500\n",
      "episode: 270   score: 138.0  q_value: [35.0620383]   memory length: 7500\n",
      "episode: 271   score: 130.0  q_value: [35.19719848]   memory length: 7500\n",
      "episode: 272   score: 154.0  q_value: [36.01955689]   memory length: 7500\n",
      "episode: 273   score: 158.0  q_value: [35.97886835]   memory length: 7500\n",
      "episode: 274   score: 127.0  q_value: [36.15120249]   memory length: 7500\n",
      "episode: 275   score: 188.0  q_value: [36.25628612]   memory length: 7500\n",
      "episode: 276   score: 123.0  q_value: [36.27125877]   memory length: 7500\n",
      "episode: 277   score: 192.0  q_value: [37.11034497]   memory length: 7500\n",
      "episode: 278   score: 152.0  q_value: [37.2206755]   memory length: 7500\n",
      "episode: 279   score: 153.0  q_value: [37.17161173]   memory length: 7500\n",
      "episode: 280   score: 141.0  q_value: [37.31313213]   memory length: 7500\n",
      "episode: 281   score: 157.0  q_value: [37.44774384]   memory length: 7500\n",
      "episode: 282   score: 169.0  q_value: [38.12601349]   memory length: 7500\n",
      "episode: 283   score: 132.0  q_value: [38.33049166]   memory length: 7500\n",
      "episode: 284   score: 145.0  q_value: [38.53979331]   memory length: 7500\n",
      "episode: 285   score: 143.0  q_value: [38.52299257]   memory length: 7500\n",
      "episode: 286   score: 139.0  q_value: [38.61015991]   memory length: 7500\n",
      "episode: 287   score: 123.0  q_value: [39.2964424]   memory length: 7500\n",
      "episode: 288   score: 193.0  q_value: [39.44297136]   memory length: 7500\n",
      "episode: 289   score: 145.0  q_value: [39.39925286]   memory length: 7500\n",
      "episode: 290   score: 164.0  q_value: [39.43959421]   memory length: 7500\n",
      "episode: 291   score: 145.0  q_value: [39.54095142]   memory length: 7500\n",
      "episode: 292   score: 153.0  q_value: [40.33151414]   memory length: 7500\n",
      "episode: 293   score: 138.0  q_value: [40.52399316]   memory length: 7500\n",
      "episode: 294   score: 138.0  q_value: [40.46937346]   memory length: 7500\n",
      "episode: 295   score: 139.0  q_value: [40.54852369]   memory length: 7500\n",
      "episode: 296   score: 150.0  q_value: [40.55305706]   memory length: 7500\n",
      "episode: 297   score: 142.0  q_value: [41.38183232]   memory length: 7500\n",
      "episode: 298   score: 121.0  q_value: [41.71808005]   memory length: 7500\n",
      "episode: 299   score: 120.0  q_value: [41.65806386]   memory length: 7500\n",
      "episode: 300   score: 112.0  q_value: [41.51102886]   memory length: 7500\n",
      "episode: 301   score: 161.0  q_value: [41.50519028]   memory length: 7500\n",
      "episode: 302   score: 133.0  q_value: [42.3470686]   memory length: 7500\n",
      "episode: 303   score: 200.0  q_value: [42.24179894]   memory length: 7500\n",
      "episode: 304   score: 156.0  q_value: [42.55488351]   memory length: 7500\n",
      "episode: 305   score: 148.0  q_value: [42.4438013]   memory length: 7500\n",
      "episode: 306   score: 181.0  q_value: [42.47541667]   memory length: 7500\n",
      "episode: 307   score: 131.0  q_value: [43.56330073]   memory length: 7500\n",
      "episode: 308   score: 158.0  q_value: [43.19167935]   memory length: 7500\n",
      "episode: 309   score: 159.0  q_value: [43.19874067]   memory length: 7500\n",
      "episode: 310   score: 160.0  q_value: [43.42597821]   memory length: 7500\n",
      "episode: 311   score: 200.0  q_value: [43.50369851]   memory length: 7500\n",
      "episode: 312   score: 155.0  q_value: [44.26121584]   memory length: 7500\n",
      "episode: 313   score: 119.0  q_value: [44.22918265]   memory length: 7500\n",
      "episode: 314   score: 146.0  q_value: [44.24916399]   memory length: 7500\n",
      "episode: 315   score: 200.0  q_value: [44.17701888]   memory length: 7500\n",
      "episode: 316   score: 200.0  q_value: [44.21325005]   memory length: 7500\n",
      "episode: 317   score: 113.0  q_value: [45.08065403]   memory length: 7500\n",
      "episode: 318   score: 104.0  q_value: [45.40156163]   memory length: 7500\n",
      "episode: 319   score: 72.0  q_value: [45.07415431]   memory length: 7500\n",
      "episode: 320   score: 159.0  q_value: [44.58324624]   memory length: 7500\n",
      "episode: 321   score: 9.0  q_value: [44.66482053]   memory length: 7500\n",
      "episode: 322   score: 25.0  q_value: [44.86904443]   memory length: 7500\n",
      "episode: 323   score: 24.0  q_value: [45.13688308]   memory length: 7500\n",
      "episode: 324   score: 130.0  q_value: [45.139669]   memory length: 7500\n",
      "episode: 325   score: 130.0  q_value: [44.32637249]   memory length: 7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 326   score: 191.0  q_value: [44.97500372]   memory length: 7500\n",
      "episode: 327   score: 25.0  q_value: [45.04201803]   memory length: 7500\n",
      "episode: 328   score: 49.0  q_value: [45.22103865]   memory length: 7500\n",
      "episode: 329   score: 165.0  q_value: [45.36574579]   memory length: 7500\n",
      "episode: 330   score: 179.0  q_value: [45.542883]   memory length: 7500\n",
      "episode: 331   score: 69.0  q_value: [45.72303801]   memory length: 7500\n",
      "episode: 332   score: 50.0  q_value: [45.68453437]   memory length: 7500\n",
      "episode: 333   score: 200.0  q_value: [45.99850381]   memory length: 7500\n",
      "episode: 334   score: 144.0  q_value: [45.96449924]   memory length: 7500\n",
      "episode: 335   score: 32.0  q_value: [46.28709475]   memory length: 7500\n",
      "episode: 336   score: 200.0  q_value: [46.07179177]   memory length: 7500\n",
      "episode: 337   score: 174.0  q_value: [45.52034069]   memory length: 7500\n",
      "episode: 338   score: 184.0  q_value: [45.72485179]   memory length: 7500\n",
      "episode: 339   score: 200.0  q_value: [46.5409373]   memory length: 7500\n",
      "episode: 340   score: 176.0  q_value: [46.32504097]   memory length: 7500\n",
      "episode: 341   score: 170.0  q_value: [46.10101669]   memory length: 7500\n",
      "episode: 342   score: 172.0  q_value: [46.58438089]   memory length: 7500\n",
      "episode: 343   score: 198.0  q_value: [46.10579185]   memory length: 7500\n",
      "episode: 344   score: 200.0  q_value: [46.56150016]   memory length: 7500\n",
      "episode: 345   score: 171.0  q_value: [46.70645136]   memory length: 7500\n",
      "episode: 346   score: 193.0  q_value: [46.79994986]   memory length: 7500\n",
      "episode: 347   score: 200.0  q_value: [47.02979384]   memory length: 7500\n",
      "episode: 348   score: 200.0  q_value: [47.02696241]   memory length: 7500\n",
      "episode: 349   score: 200.0  q_value: [47.00042186]   memory length: 7500\n",
      "episode: 350   score: 200.0  q_value: [46.6436999]   memory length: 7500\n",
      "episode: 351   score: 200.0  q_value: [47.0258233]   memory length: 7500\n",
      "episode: 352   score: 200.0  q_value: [47.56409507]   memory length: 7500\n",
      "episode: 353   score: 200.0  q_value: [47.48153412]   memory length: 7500\n",
      "episode: 354   score: 200.0  q_value: [46.66438605]   memory length: 7500\n",
      "episode: 355   score: 200.0  q_value: [47.65567493]   memory length: 7500\n",
      "episode: 356   score: 163.0  q_value: [47.85286094]   memory length: 7500\n",
      "episode: 357   score: 200.0  q_value: [48.989812]   memory length: 7500\n",
      "episode: 358   score: 200.0  q_value: [48.21106608]   memory length: 7500\n",
      "episode: 359   score: 173.0  q_value: [48.53050313]   memory length: 7500\n",
      "episode: 360   score: 200.0  q_value: [48.84139968]   memory length: 7500\n",
      "episode: 361   score: 200.0  q_value: [49.21927029]   memory length: 7500\n",
      "episode: 362   score: 200.0  q_value: [49.27484149]   memory length: 7500\n",
      "episode: 363   score: 200.0  q_value: [49.73019706]   memory length: 7500\n",
      "episode: 364   score: 200.0  q_value: [48.70794391]   memory length: 7500\n",
      "episode: 365   score: 200.0  q_value: [49.23036008]   memory length: 7500\n",
      "episode: 366   score: 200.0  q_value: [49.15331252]   memory length: 7500\n",
      "episode: 367   score: 143.0  q_value: [50.60988307]   memory length: 7500\n",
      "episode: 368   score: 200.0  q_value: [50.86352739]   memory length: 7500\n",
      "episode: 369   score: 175.0  q_value: [50.68325333]   memory length: 7500\n",
      "episode: 370   score: 200.0  q_value: [49.92490338]   memory length: 7500\n",
      "episode: 371   score: 200.0  q_value: [49.66534924]   memory length: 7500\n",
      "episode: 372   score: 200.0  q_value: [51.02676853]   memory length: 7500\n",
      "episode: 373   score: 200.0  q_value: [49.54338743]   memory length: 7500\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "EPISODES = 1000 #Maximum number of episodes 1000\n",
    "\n",
    "#DQN Agent for the Cartpole\n",
    "#Q function approximation with NN, experience replay, and target network\n",
    "class DQNAgent:\n",
    "    #Constructor for the agent (invoked when DQN is first called in main)\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.check_solve = True\t#If True, stop if you satisfy solution condition\n",
    "        self.render = True       #If you want to see Cartpole learning, then change to True\n",
    "\n",
    "        #Get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "       # Modify here\n",
    "\n",
    "        #Set hyper parameters for the DQN. Do not adjust those labeled as Fixed.\n",
    "        self.discount_factor = 0.995#0.95\n",
    "        self.learning_rate = 0.005#0.005\n",
    "        self.epsilon = 0.02 #Fixed\n",
    "        self.batch_size = 32 #Fixed\n",
    "        self.memory_size = 7500#1000\n",
    "        self.train_start = 1000 #Fixed\n",
    "        self.target_update_frequency = 1#1\n",
    "\n",
    "        #Number of test states for Q value plots\n",
    "        self.test_state_no = 10000\n",
    "\n",
    "        #Create memory buffer using deque\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "\n",
    "        #Create main network and target network (using build_model defined below)\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "        #Initialize target network\n",
    "        self.update_target_model()\n",
    "\n",
    "    #Approximate Q function using Neural Network\n",
    "    #State is the input and the Q Values are the output.\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Edit the Neural Network model here\n",
    "        #Tip: Consult https://keras.io/getting-started/sequential-model-guide/\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform')) # 16\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "    #After some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    #Get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "        #Insert your e-greedy policy code here\n",
    "        #Tip 1: Use the random package to generate a random action.\n",
    "        #Tip 2: Use keras.model.predict() to compute Q-values from the state.\n",
    "        \n",
    "        ''' So E Greedy is like, you take a random action with proabability epsilon, other with 1-e\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        q_val = self.model.predict(state)\n",
    "        #print(\"Something is fishy!\")\n",
    "        #print(q_val)\n",
    "        best_action = np.argmax(q_val);\n",
    "        #print(\" I choose \",best_action)\n",
    "        rand_act = random.randrange(self.action_size);\n",
    "        #print(\" The random value might be \", rand_act);\n",
    "        \n",
    "        \n",
    "        return np.random.choice([rand_act,best_action],p=[self.epsilon,1-self.epsilon]);\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "    #Save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) #Add sample to the end of the list\n",
    "\n",
    "    #Sample <s,a,r,s'> from replay memory\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start: #Do not train if not enough memory\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory)) #Train on at most as many samples as you have in memory\n",
    "        mini_batch = random.sample(self.memory, batch_size) #Uniformly sample the memory buffer\n",
    "        #Preallocate network and target network input matrices.\n",
    "        update_input = np.zeros((batch_size, self.state_size)) #batch_size by state_size two-dimensional array (not matrix!)\n",
    "        update_target = np.zeros((batch_size, self.state_size)) #Same as above, but used for the target network\n",
    "        action, reward, done = [], [], [] #Empty arrays that will grow dynamically\n",
    "        y_batch = []#I Created this array\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0] #Allocate s(i) to the network input array from iteration i in the batch\n",
    "            action.append(mini_batch[i][1]) #Store a(i)\n",
    "            reward.append(mini_batch[i][2]) #Store r(i)\n",
    "            update_target[i] = mini_batch[i][3] #Allocate s'(i) for the target network array from iteration i in the batch\n",
    "            done.append(mini_batch[i][4])  #Store done(i)\n",
    "\n",
    "        target = self.model.predict(update_input) #Generate target values for training the inner loop network using the network model\n",
    "        target_val = self.target_model.predict(update_target) #Generate the target values for training the outer loop target network\n",
    "        #print(\"Target :\",target,\"Target_val :\",target_val)            \n",
    "        \n",
    "       # Added by me \n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size): # Setting yj and gradient descent  line 12-13\n",
    "        # if terminal only equals reward\n",
    "            if mini_batch[i][4]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] =  reward[i]+ self.discount_factor * np.max(target_val[i])\n",
    "            \n",
    "        #print(\"Q values in target\",target)\n",
    "        #Q Learning: get maximum Q value at s' from target network\n",
    "###############################################################################\n",
    "###########################x####################################################\n",
    "        #Insert your Q-learning code here\n",
    "        #Tip 1: Observe that the Q-values are stored in the variable target\n",
    "        #Tip 2: What is the Q-value of the action taken at the last state of the episode?\n",
    "        # print(target)\n",
    "        \n",
    "        \n",
    "#         for i in range(self.batch_size): #For every batch\n",
    "#             if mini_batch[i][4]:\n",
    "#                target[i][action[i]] = 1\n",
    "#             target[i][action[i]] += self.learning_rate *(reward[i] + (self.discount_factor * max(target[i]))- target[i][action[i]]) #random.randint(0,1)\n",
    "# ###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "        #Train the inner loop network\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n",
    "        return\n",
    "    #Plots the score per episode as well as the maximum q value per episode, averaged over precollected states.\n",
    "    def plot_data(self, episodes, scores, max_q_mean):\n",
    "        pylab.figure(0)\n",
    "        pylab.plot(episodes, max_q_mean, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Average Q Value\")\n",
    "        pylab.savefig(\"qvalues.png\")\n",
    "\n",
    "        pylab.figure(1)\n",
    "        pylab.plot(episodes, scores, 'b')\n",
    "        pylab.xlabel(\"Episodes\")\n",
    "        pylab.ylabel(\"Score\")\n",
    "        pylab.savefig(\"scores.png\")\n",
    "\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #For CartPole-v0, maximum episode length is 200\n",
    "    env = gym.make('CartPole-v0') #Generate Cartpole-v0 environment object from the gym library\n",
    "    #Get state and action sizes from the environment\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    #Create agent, see the DQNAgent __init__ method for details\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "    #Collect test states for plotting Q values using uniform random policy\n",
    "    test_states = np.zeros((agent.test_state_no, state_size))\n",
    "    max_q = np.zeros((EPISODES, agent.test_state_no))\n",
    "    max_q_mean = np.zeros((EPISODES,1))\n",
    "\n",
    "    done = True\n",
    "    for i in range(agent.test_state_no):\n",
    "        if done:\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "        else:\n",
    "            action = random.randrange(action_size)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            test_states[i] = state\n",
    "            state = next_state\n",
    "\n",
    "    scores, episodes = [], [] #Create dynamically growing score and episode counters\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset() #Initialize/reset the environment\n",
    "        state = np.reshape(state, [1, state_size]) #Reshape state so that to a 1 by state_size two-dimensional array ie. [x_1,x_2] to [[x_1,x_2]]\n",
    "        #Compute Q values for plotting\n",
    "        tmp = agent.model.predict(test_states)\n",
    "        max_q[e][:] = np.max(tmp, axis=1)\n",
    "        max_q_mean[e] = np.mean(max_q[e][:])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render() #Show cartpole animation\n",
    "\n",
    "            #Get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state) # Selecting the action by epsilon greedy -sab #Line 6 and 7\n",
    "            next_state, reward, done, info = env.step(action) # Executing the action  - sab # Line 8\n",
    "            next_state = np.reshape(next_state, [1, state_size]) #Reshape next_state similarly to state # maybe line 9\n",
    "\n",
    "            #Save sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)  # this looks like the storing transition in D - sab # Line 10\n",
    "            #Training step\n",
    "            agent.train_model()\n",
    "            score += reward #Store episodic reward\n",
    "            state = next_state #Propagate state\n",
    "\n",
    "            if done:\n",
    "                #At the end of very episode, update the target network\n",
    "                if e % agent.target_update_frequency == 0:\n",
    "                    agent.update_target_model()\n",
    "                #Plot the play time for every episode\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "\n",
    "                print(\"episode:\", e, \"  score:\", score,\" q_value:\", max_q_mean[e],\"  memory length:\",\n",
    "                      len(agent.memory))\n",
    "\n",
    "                # if the mean of scores of last 100 episodes is bigger than 195\n",
    "                # stop training\n",
    "                if agent.check_solve:\n",
    "                    if np.mean(scores[-min(100, len(scores)):]) >= 195:\n",
    "                        print(\"solved after\", e-100, \"episodes\")\n",
    "                        agent.plot_data(episodes,scores,max_q_mean[:e+1])\n",
    "                        sys.exit()\n",
    "    agent.plot_data(episodes,scores,max_q_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_data(episodes,scores,max_q_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
