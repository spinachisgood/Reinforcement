{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the preparatory lab \n",
    "In this notebook, we use the following modules `numpy` and `maze`. The latter is a home made module, where all the solutions to the questions are implemented. We will refer to it at each answer, and we encourage you to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import maze as mz \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Shortest path in the maze\n",
    "\n",
    "The objective of problem 1 is to solve the shortest path problem in a maze. We start first by describing the maze as a numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 2, 0]\n",
    "])\n",
    "# with the convention \n",
    "# 0 = empty cell\n",
    "# 1 = obstacle\n",
    "# 2 = exit of the Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `maze.draw_maze()` helps us draw the maze given its numpy array discription.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mz.draw_maze(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP formulation\n",
    "\n",
    "We propose the following MDP formulation: \n",
    "\n",
    "#### State space $\\mathcal{S}$\n",
    "We model the state space as the set of all possible positions of the player in the maze. Note that we exclude the obstacles' position since these are impossible states to be in for the player. Formally, the state space is\n",
    "\n",
    "$$\\mathcal{S} = \\big\\lbrace (i,j):\\textrm{such that the cell\n",
    "} (i,j) \\textrm{ is not an obstacle}\\big\\rbrace.$$\n",
    "> **Note:** The choice of state space is not unique. For instance one could consider $\\mathcal{S}$ to be the set of all positions in the  maze regardless of whether they correspond to an obstacle or not. But note that, this will increase the size $\\vert \\mathcal{S} \\vert $. This is fine for small mazes, but it leads to many redundant states as the maze dimension increases.\n",
    "\n",
    "#### Action space $\\mathcal{A}$\n",
    "We allow the player to chose to either move `left`, `right`, `down`, `up` or not move at all (`stay`). Note that sometimes the player cannot move in a certain direction because of an obstacle or a wall, yet we permit this to be action. We will see that this is not an issue as long as we define our transition probabilities and rewards appropriately.\n",
    "Formally, the action space is\n",
    "\n",
    "$$\\mathcal{A} = \\lbrace \\textrm{up}, \\textrm{ down}, \\textrm{ left}, \\textrm{ right}, \\textrm{ stay} \\rbrace.$$\n",
    "> **Note:** Once again, the choice of the action space is not unique. For instance one could remove the action `stay` from $\\mathcal{A}$, but then one should modify the transition probabilities accordingly as well as the rewards.  \n",
    "\n",
    "\n",
    "#### Transition probabilities $\\mathcal{P}$\n",
    "Note that there is no randomness involved upon taking an action by the player. As a consequence, the transition probabilities are deterministic. More precisely,   \n",
    "- If at state (or position) $s$ taking action (or move) $a$ does not lead to a wall or an obstacle but to another state (or position) $s'$, then $\\mathbb{P}(s' \\vert s, a) = 1$. \n",
    "- If at state (or position)  $s$ taking action (or move) $a$ leads to a wall or an obstacle, the player remains in his state (or position) $s$, then $\\mathbb{P}(s \\vert s, a) = 1$.\n",
    "\n",
    "> **Note**: Recall that for a fixed $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$ we have $\\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s' \\vert s, a) = 1$, thus if for some $s' \\in \\mathcal{S}$  we have $\\mathbb{P}(s' \\vert s, a) = 1$, then for all $s'' \\in \\mathcal{S} \\backslash \\lbrace s'\\rbrace$ we have $\\mathbb{P}(s'' \\vert s, a) = 0$,\n",
    "\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads to a wall or an obstacle then $r(s,a) = -\\infty$\n",
    "   - If at state $s$, taking action $a$, leads to some other position in the maze that is not the exit nor a wall nor an obstacle, then $r(s, a) = -1$. \n",
    "   - If at state $s$, taking action $a$, leads to the exit then $r(s ,a) = 0$. \n",
    "> **Note**: Here the rewards are independent of time (i.e. $r_t(.,.) = r(.,.)$). \n",
    "\n",
    "\n",
    "### Implementation\n",
    "The above MDP formulation is implemented as a class ``maze.Maze`` in the file [maze.py](./maze.py) which given a matrix description of the maze instanciates the state space, action space, transition probabilities and rewards. \n",
    "\n",
    "> **Note:** In the class `maze.Maze` each state $s = (i,j)$ is given a unique identifier $s_{id} \\in \\lbrace 0, , \\dots, \\vert S \\vert -1 \\rbrace$. In other words, the state space from an implementation perspective is viewed as the set of integers $\\lbrace 0, , \\dots, \\vert S \\vert -1 \\rbrace$. This mapping is done via the dictionary `self.map` and its inverse mapping via the dictionary `self.states`.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dynamic Programming \n",
    "\n",
    "Before solving the MDP problem, recall that the finite horizon objective function is \n",
    "$$\n",
    "    \\mathbb{E} \\Big[ \\sum_{t=0}^T r(s_t, a_t) \\Big],\n",
    "$$\n",
    "where $T$ is the horizon.\n",
    "Recall the Bellman equation \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V(s) = \\max_{a \\in \\mathcal{A}} \\Big\\lbrace r(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,a) V(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "The dynamic programming solution for the finite horizon MDP problem consists of solving the above backward recursion. The method `maze.dynamic_programming` achieves this. \n",
    "> **Note:** To find the optimal path, it is enough to set the time horizon $T = 10$. Indeed, looking at the maze one can see that the player needs at least 10 steps to attain the exit $B$, if her starting position is at $A$. In fact if you set the time horizon less than 10, you will see that you do not find the optimal path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 10\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Here we solve the discounted infinite-horizon MDP problem using value iteration, the objective here is to find a stationary policy $\\pi$ that minimizes the infinite horizon objective with a discount factor $\\gamma$ \n",
    "$$\n",
    "    \\mathbb{E} \\Big[\\sum_{t=0}^\\infty \\gamma^t r\\big(s_t, \\pi(s_t)\\big) \\Big].\n",
    "$$\n",
    "Recall the Bellman equation in the case of a stationary policy $\\pi$ \n",
    "\\begin{equation}\n",
    "\\forall s \\in \\mathcal{S} \\qquad  V^*(s) = \\max_{\\pi} \\Big\\lbrace r(s,\\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,\\pi(s)) V^*(s') \\Big\\rbrace\n",
    "\\end{equation}\n",
    "or equivalently in terms of the Bellman operator $\\mathcal{L}$ \n",
    "\\begin{equation}\n",
    "V^* =  \\mathcal{L}(V^*)\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{equation}\n",
    "   \\forall s \\in \\mathcal{S} \\qquad  \\mathcal{L}(V)(s) = \\max_{\\pi} \\Big\\lbrace r(s,\\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathbb{P}(s'\\vert s,\\pi(s)) V(s') \\Big\\rbrace. \n",
    "\\end{equation}\n",
    "Value iteration solves the Bellman equation described above. This method is implemented as `maze.value_iteration` in the file [maze.py]().\n",
    "\n",
    "> **Note:** Recall that the mapping $\\mathcal{L}$ is a contraction, therefore value iteration converges. To achieve an $\\varepsilon>0$ approximation (i.e. $\\Vert V^* - V_{n+1} \\Vert \\le \\varepsilon$),\n",
    " the stopping criterion of value iteration is $\\Vert V - \\mathcal{L}(V) \\Vert < \\frac{1-\\gamma}{\\gamma}\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.95; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random rewards \n",
    "\n",
    "### The new MDP formulation \n",
    "As stated in the problem statement, we only modify the rewards $\\mathcal{R}$ to be random. In fact we will only need to modify the rewards corresponding to the state action pair $(s,a)$ that lead to either the cell R1 or R2.\n",
    "#### Rewards $\\mathcal{R}$\n",
    "The objective of the player is to find the exit of the maze while avoiding the obstacles.    \n",
    "   - If at state $s$, taking action $a$, leads to the cell R1 then the reward is random according to the following     $$ R(s,a) = \\begin{cases}\n",
    "            -7 \\quad \\textrm{ w.p. } 0.5 \\\\\n",
    "            -1 \\quad \\textrm{ w.p. } 0.5\n",
    "            \\end{cases} \n",
    "     $$\n",
    "   - If at state $s$, taking action $a$, leads to the cell R2 then the reward is random according to the following \n",
    "     $$ R(s,a) = \\begin{cases}\n",
    "            -2 \\quad \\textrm{ w.p. } 0.5 \\\\\n",
    "            -1 \\quad \\textrm{ w.p. } 0.5\n",
    "            \\end{cases} \n",
    "     $$\n",
    "   - The remaining rewards remain deterministic and with the same values as in the previous formulation.\n",
    "\n",
    "> **Note**: The fact that you stay in a cell for a number of rounds $n$ means that you are forced to incur the reward of ending up in that state for an additional $n$ times. Thus, instead of modifying the transition probabilities, we can modify the reward of ending up at that round by multiplying it by $n + 1$.  \n",
    "\n",
    "### Solving the new MDP \n",
    "As mentioned in the appendix [random_rewards.pdf]() (see in canvas), when solving the problem we will only have to look at the average rewards instead of the realization of the rewards, and the methods implemented for the previous case remain unchanged.  \n",
    "\n",
    "> **Note**: In the implementation, the only change will be the rewards. In addition, the policies we obtain remain deterministic. However, when running a policy the accumulated reward is random, but its average over multiple repetitions should converge to the value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 0, 0, 0, 0, -1],\n",
    "    [ 0, 1, 1, 1, 1, 1,  0],\n",
    "    [-6, 0, 0, 0, 0, 2,  0]\n",
    "])\n",
    "# with the convention \n",
    "#  0 = empty cell\n",
    "#  1 = obstacle\n",
    "#  2 = exit of the Maze\n",
    "# -n = trapped cell with probability 0.5. If the cell is trapped the player must stay there for n times.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mz.draw_maze(maze);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic programming \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze, random_rewards=True)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 15\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method);\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The animation does not illustrate the event where the player is trapped as it assumes average rewards. Nonetheless, the shown policy is the optimal one.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration  \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.95; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.0001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)\n",
    "\n",
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 : Plucking berries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The new MDP formulation \n",
    "\n",
    "In this problem, the introduction of weights is translated in our previous MDP formulation by a modification of the rewards $\\mathcal{R}$. This is done by simply setting $r(s,a)$ to $w_{ij}$ if being in state $s$ and taking action $a$ leads to being in th new state $s'=(i,j)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 1, 0, 0, 0,  0],\n",
    "    [ 0, 0, 0, 0, 0, 0,  0],\n",
    "    [ 0, 1, 1, 1, 1, 1,  0],\n",
    "    [ 0, 0, 0, 0, 0, 2,  0]\n",
    "])\n",
    "\n",
    "# Description of the weight matrix as a numpy array\n",
    "w = np.array([\n",
    "    [0,    1, -100,   10,   10,   10, 10],\n",
    "    [0,    1, -100,   10,    0,    0, 10],\n",
    "    [0,    1, -100,   10,    0,    0, 10],\n",
    "    [0,    1,    1,    1,    0,    0, 10],\n",
    "    [0, -100, -100, -100, -100, -100, 10],\n",
    "    [0,    0,    0,    0,    0,   11, 10]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an environment maze\n",
    "env = mz.Maze(maze, weights=w)\n",
    "# env.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic programming \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with dynamic programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming \n",
    "V, policy= mz.dynamic_programming(env,horizon);\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method);\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** By changing the horizon from $20$ to $12$ you should observe that the optimal policy changes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration  \n",
    "\n",
    "Run the following python code to obtain the optimal solution of the newly formulated MDP with value iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount Factor \n",
    "gamma   = 0.50; \n",
    "# Accuracy treshold \n",
    "epsilon = 0.001;\n",
    "V, policy = mz.value_iteration(env, gamma, epsilon)\n",
    "method = 'ValIter';\n",
    "start  = (0,0);\n",
    "path = env.simulate(start, policy, method)\n",
    "# Show the shortest path \n",
    "mz.animate_solution(maze, path)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
